{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 5: Model Evaluation 2 -- Cross-Validation for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "## K-fold Cross-Validation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple demonstration of using a cross-validation iterator in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " [[0.69646919 0.28613933 0.22685145 0.55131477]\n",
      " [0.71946897 0.42310646 0.9807642  0.68482974]\n",
      " [0.4809319  0.39211752 0.34317802 0.72904971]\n",
      " [0.43857224 0.0596779  0.39804426 0.73799541]\n",
      " [0.18249173 0.17545176 0.53155137 0.53182759]\n",
      " [0.63440096 0.84943179 0.72445532 0.61102351]\n",
      " [0.72244338 0.32295891 0.36178866 0.22826323]\n",
      " [0.29371405 0.63097612 0.09210494 0.43370117]\n",
      " [0.43086276 0.4936851  0.42583029 0.31226122]\n",
      " [0.42635131 0.89338916 0.94416002 0.50183668]]\n",
      "Labels:\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      "\n",
      "\n",
      "Example indices in fold (training/validation): (array([2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 4, 5, 6, 7, 8, 9]), array([2, 3]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 2, 3, 6, 7, 8, 9]), array([4, 5]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 2, 3, 4, 5, 8, 9]), array([6, 7]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 2, 3, 4, 5, 6, 7]), array([8, 9]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set random generator with some number so that we get the same results when we re-run the code\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "# we will use a random dataset for simplicity\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) # example inputs: 10 x 4 dimensional dataset\n",
    "X = rng.random_sample((y.shape[0], 4))\n",
    "\n",
    "# Print dataset\n",
    "print(\"Features:\\n\", X)\n",
    "print(\"Labels:\\n\", y)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "print(\"\\n\")\n",
    "for k in cv.split(X, y):\n",
    "    print(\"Example indices in fold (training/validation):\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4809319 , 0.39211752, 0.34317802, 0.72904971],\n",
       "       [0.43857224, 0.0596779 , 0.39804426, 0.73799541],\n",
       "       [0.18249173, 0.17545176, 0.53155137, 0.53182759],\n",
       "       [0.63440096, 0.84943179, 0.72445532, 0.61102351],\n",
       "       [0.72244338, 0.32295891, 0.36178866, 0.22826323],\n",
       "       [0.29371405, 0.63097612, 0.09210494, 0.43370117],\n",
       "       [0.43086276, 0.4936851 , 0.42583029, 0.31226122],\n",
       "       [0.42635131, 0.89338916, 0.94416002, 0.50183668]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training examples first fold\n",
    "X[[2, 3, 4, 5, 6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training labels first fold\n",
    "y[[2, 3, 4, 5, 6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69646919, 0.28613933, 0.22685145, 0.55131477],\n",
       "       [0.71946897, 0.42310646, 0.9807642 , 0.68482974]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation examples first fold\n",
    "X[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation labels first fold\n",
    "y[[0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- In practice, we are usually interested in shuffling the dataset, because if the data records are ordered by class label, this would result in cases where the classes are not well represented in the training and test folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example indices in fold (training/validation): (array([1, 2, 3, 5, 6, 7, 8, 9]), array([0, 4]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 2, 3, 4, 6, 8, 9]), array([5, 7]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 2, 4, 5, 6, 7, 9]), array([3, 8]))\n",
      "Example indices in fold (training/validation): (array([0, 2, 3, 4, 5, 7, 8, 9]), array([1, 6]))\n",
      "Example indices in fold (training/validation): (array([0, 1, 3, 4, 5, 6, 7, 8]), array([2, 9]))\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for k in cv.split(X, y):\n",
    "    print(\"Example indices in fold (training/validation):\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Note that the `KFold` iterator only provides us with the array indices; in practice, we are actually interested in the array values (feature values and class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels with shuffling [0 0 0 1 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 0 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n",
      "validation labels with shuffling [0 0]\n",
      "validation labels with shuffling [1 1]\n",
      "validation labels with shuffling [0 1]\n",
      "validation labels with shuffling [0 1]\n",
      "validation labels with shuffling [0 1]\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels with shuffling', y[train_idx])\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('validation labels with shuffling', y[valid_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- It's important to stratify the splits (very crucial for small datasets!). Observe that the distribution of labels in the training and validation parts of each fold is well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "validation labels [0 1]\n",
      "validation labels [0 1]\n",
      "validation labels [0 1]\n",
      "validation labels [0 1]\n",
      "validation labels [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels', y[train_idx])\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('validation labels', y[valid_idx])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "## Cross-validation for model evalution: Logistic regression\n",
    "\n",
    "- After the illustrations of cross-validation above, the next cell demonstrates how we can actually use the iterators provided through scikit-learn to fit and evaluate a learning algorithm. We start by using cross-validation to evaluate a logistic regression model.\n",
    "-  Recall that feature scaling DOES matter for most machine learning algorithms, including logistic regression.\n",
    "- Moreover, it is compulsory if we do regularisation.\n",
    "- To avoid introducing bias, we have to compute the parameters for scaling (e.g., the mean and standard deviation in the context of z-score normalisation) on the training fold to scale the training AND test fold in a given iteration.\n",
    "- To make this more convenient, this is where scikit-learn's `Pipeline` class (or `make_pipeline` function) comes in handy, as the next cell demonstrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_acc: 0.9890 ± 0.0000\n",
      "  test_acc: 0.9780 ± 0.0098\n",
      " train_nll: 0.0509 ± 0.0063\n",
      "  test_nll: 0.0721 ± 0.0287\n",
      "\n",
      "Final hold-out test metrics:\n",
      "Accuracy : 0.9825\n",
      "Log loss : 0.0779\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Load data and split into training and test set\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Hold-out test split for final unbiased report\n",
    "# stratify tells train_test_split to preserve the class proportions of the input labels in both the train and test sets.\n",
    "# Use stratify=y (where y are your class labels) to get splits with roughly the same class distribution as the full dataset.\n",
    "# Helpful for imbalanced datasets so the test (and train) sets aren’t skewed.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: scale > logistic regression\n",
    "# We cannot scale data before cross-validation. If you scale before CV on the whole dataset, you cause data leakage. \n",
    "# Using a Pipeline avoids that automatically.\n",
    "# In cross-validation, scikit-learn scales separately on each training fold. \n",
    "# On each fold, scaling runs only on that fold’s training data, learning its μ and σ.\n",
    "# The scaler then transforms the fold’s validation data using those μ and σ.\n",
    "# No information from the validation fold leaks into the scaler or model.\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(solver=\"lbfgs\", max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validation on the training set\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"nll\": \"neg_log_loss\"\n",
    "}\n",
    "cv_res = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Summarise CV results\n",
    "for k in [\"train_acc\", \"test_acc\", \"train_nll\", \"test_nll\"]:\n",
    "    mean = cv_res[k].mean()\n",
    "    std  = cv_res[k].std()\n",
    "    # For log loss, we stored negative log loss; flip sign when printing\n",
    "    if \"nll\" in k:\n",
    "        mean, std = -mean, std\n",
    "    print(f\"{k:>10}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "# Fit on the whole training set because we need a \"single\" model and evaluate once on the hold-out test set\n",
    "# Step B in the slide \"k-fold cross-validation for model evaluation\"\n",
    "pipe.fit(X_train, y_train)\n",
    "p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "y_pred = (p_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nFinal hold-out test metrics:\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Log loss : {log_loss(y_test, p_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "\n",
    "- In the example above, we set the following hyperparameters for logistic regression:\n",
    "    - solver=\"lbfgs\": Optimisation algorithm. Supports multinomial logistic regression and works well for medium-sized datasets.\n",
    "    - max_iter=5000: Maximum number of iterations.\n",
    "- Keep in mind that `cross_validate` **does not tune hyperparameters**. It simply evaluates the model with the hyperparameters you give it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:3cm;\"></p>\n",
    "\n",
    "- Usually, a more convenient way to use cross-validation through scikit-learn is to use the `cross_val_score` function (note that it performs stratified splitting for classification by default)\n",
    "- We use `cross_val_score` when we only need a single metric (e.g., accuracy) for cross-validation. We use `cross_validate` (logistic regression example above) when we require multiple metrics or/and more detailed evaluation and diagnostics. The latter returns a dict of results, which can include: training scores, multiple metrics, fit times and score times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use `cross_val_score` to do cross-validation for our example considering logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.9780 ± 0.0098\n",
      "Test accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data + hold-out split\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: scale -> logistic regression\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(solver=\"lbfgs\", max_iter=5000, random_state=42)),\n",
    "])\n",
    "\n",
    "# 5-fold CV accuracy on the training set\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_acc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "print(f\"CV accuracy: {cv_acc.mean():.4f} ± {cv_acc.std():.4f}\")\n",
    "\n",
    "# Fit on all training data and evaluate on hold-out test set\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation for model evaluation: Decision trees\n",
    "\n",
    "- Consider now cross-validation computed by hand for a decision tree for the Iris dataset, which is non-binary; it has three classes: (['setosa' 'versicolor' 'virginica'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 95.26%\n",
      "Test Accuracy: 95.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15, \n",
    "                                                    shuffle=True, stratify=y)\n",
    "\n",
    "\n",
    "# Step A in the slide \"k-fold cross-validation for model evaluation\"\n",
    "# k = 10 as recommended in: Kohavi, R. (1995, August). A study of cross-validation and bootstrap for accuracy estimation and model selection. In IJCAI  (Vol. 14, No. 2, pp. 1137-1145).\n",
    "cv = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "kfold_acc = 0.\n",
    "# For each training, validation set in each fold\n",
    "for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "    # Train a decision tree in the training fold\n",
    "    clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train[train_idx], y_train[train_idx])\n",
    "    # Step C in the slide \"k-fold cross-validation for model evaluation\"\n",
    "    # Predict on the validation fold\n",
    "    y_pred = clf.predict(X_train[valid_idx])\n",
    "    # Compute accuracy of the current fold\n",
    "    acc = np.mean(y_pred == y_train[valid_idx])*100\n",
    "    # Accumulate fold performances (accuracies)\n",
    "    kfold_acc += acc\n",
    "# Compute the estimate of the generalisation performance\n",
    "kfold_acc /= 10\n",
    "\n",
    "# Fit a new decision tree with all training data because we need a \"single\" model\n",
    "# Step B in the slide \"k-fold cross-validation for model evaluation\"\n",
    "clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = np.mean(y_pred == y_test)*100\n",
    "    \n",
    "print('Kfold Accuracy: %.2f%%' % kfold_acc)\n",
    "print('Test Accuracy: %.2f%%' % test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use `cross_val_score` to do cross-validation for our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 96.09%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=-1) # means use all processors to do the training and validation over folds in parallel, provided there are several processors\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))\n",
    "# The result will be different because we cannot set a random seed for cross_val_score as we did above for StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- `cross_val_score` has unfortunately no way to specify a random seed. This is not an issue in regular use cases, but it is not useful if you want to do \"repeated cross-validation\". Repeated cross-validation runs a cross-validation scheme multiple times with different random splits, then aggregates (mean/SD) the scores.\n",
    "- The next cell illustrates how we can provide our own cross-validation iterator for convenience (note that the results match our \"manual\" `StratifiedKFold` approach we performed earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 95.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
