{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e98701-3c8d-4d99-ac0a-086d57c0a659",
   "metadata": {},
   "source": [
    "## LECTURE 8. CLASS EXERCISE: Boosting Algorithm Training Time Showdown\n",
    "\n",
    "**Objective:**  \n",
    "Compare the training time required by scikit-learn's `GradientBoostingClassifier` and `HistGradientBoostingClassifier`, XGBoost (`XGBClassifier`), and LightGBM (`LGBMClassifier`).\n",
    "\n",
    "\n",
    "**Dataset Choice:**  \n",
    "\n",
    "We will use the **Covertype** dataset, which contains **581,012 samples** and **54 features**, making it sufficiently challenging to highlight differences in algorithm efficiency. It is a moderately sized dataset that is large enough to show a noticeable difference in training times but small enough to run quickly within a class session. \n",
    "\n",
    "The Covertype dataset contains forest cover type labels for 30Ã—30 m plots in Colorado, utilising 54 cartographic and environmental features (elevation, slope, soil type, etc.) to predict one of seven forest cover classes. Therefore, the task is to predict the forest Cover_Type (the primary tree species, e.g., Spruce/Fir, Lodgepole Pine, Ponderosa Pine) for specific 30 x 30 meter areas of land.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a80bf3-88ac-4df8-9af9-b2dc5412c8c3",
   "metadata": {},
   "source": [
    "**Step 1: Setup the Environment**  \n",
    "Install `xgboost` and `lightgbm` as they are not pre-installed in Colab's default environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3da2c86a-d165-4f1b-a07f-52f1557e9572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.0.2)\n",
      "Requirement already satisfied: lightgbm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.23.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.9.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install external libraries in Google Colab\n",
    "!pip install xgboost lightgbm\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e6f4d-f7c9-4d21-aa73-2d13893fe923",
   "metadata": {},
   "source": [
    "**Step 2: Load and Preprocess the Dataset**  \n",
    "The Covertype dataset needs to be fetched and prepared. Scaling the features help ensure fair comparisons, especially for `GradientBoostingClassifier`, which benefits from it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8461ba-e8a0-4baa-9646-ab4155e0b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded: 46480 training samples, 54 features.\n"
     ]
    }
   ],
   "source": [
    "# Load the Covertype dataset\n",
    "\n",
    "#import ssl\n",
    "#ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "print(\"Loading data...\")\n",
    "covertype = fetch_covtype(shuffle=True, random_state=42)\n",
    "X, y = covertype.data, covertype.target\n",
    "\n",
    "# --- LABEL INDEXING  ---\n",
    "y = y - 1 \n",
    "# The classes are now [0, 1, 2, 3, 4, 5, 6] instead of [1, 2, 3, 4, 5, 6, 7]\n",
    "# -----------------------------------\n",
    "\n",
    "# Use a smaller subset for faster execution in class, if needed!!!\n",
    "# We use only 10% of the dataset here as training data\n",
    "# Because train_size is 0.1 (10%), the remaining 90% goes into the \"test\" portion\n",
    "X, _, y, _ = train_test_split(X, y, train_size=0.1, random_state=42, stratify=y) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Data loaded: {X_train_scaled.shape[0]} training samples, {X_train_scaled.shape[1]} features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d62c9d-6c82-41d1-bb56-dc17063f73b9",
   "metadata": {},
   "source": [
    "**Step 3: Define Models and a Timing Function**  \n",
    "Define the classifiers with consistent parameters where possible, and create a function to time the training process cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe7e7a8-45b7-4b9a-b10b-745cd4afdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the models with basic, comparable parameters\n",
    "# We limit n_estimators to a reasonable number to keep runtime manageable for the class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe79acc-ec6d-4c0d-985e-33d4399727e4",
   "metadata": {},
   "source": [
    "**Step 4: Run the Comparison**  \n",
    "Execute the training for all four models and collect their training times and accuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedae3c4-45ee-4220-bfae-1a5a4dcac134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a975e8c-b2ef-4735-9970-0ef9dd0a65a7",
   "metadata": {},
   "source": [
    "**Step 5: Analyse and Visualise Results**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c7424-6669-4235-93ea-6468dc81dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "# based on your own code above\n",
    "summary_df = pd.DataFrame({\n",
    "    'Algorithm': list(timing_results.keys()),\n",
    "    'Training Time (s)': list(timing_results.values()),\n",
    "    'Accuracy': list(accuracy_results.values())\n",
    "}).sort_values(by='Training Time (s)')\n",
    "\n",
    "print(\"\\n--- Summary Comparison ---\")\n",
    "print(summary_df.round(4))\n",
    "\n",
    "# Optional: Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565601d-6c7e-4c05-8540-1d4e79ab52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
