{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb38f9a8",
   "metadata": {},
   "source": [
    "Volledig nieuwe poging to preproc na tips van guillem  (28/11 21:19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306b9236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE PREPROCESSING PIPELINE - FINAL VERSION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL PREPROCESSING PIPELINE - COMPREHENSIVE & VALIDATED\n",
    "# Built with all lessons learned + TA hints\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE PREPROCESSING PIPELINE - FINAL VERSION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8696a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded successfully:\n",
      "  Train: (20885, 44)\n",
      "  Test: (5221, 39)\n",
      "  Diagnoses: (651047, 4)\n",
      "\n",
      "✓ Column names standardized\n",
      "\n",
      "--- Dataset Overview ---\n",
      "Train samples: 20,885\n",
      "Test samples: 5,221\n",
      "Diagnosis records: 651,047\n",
      "\n",
      "--- Duplicate Check ---\n",
      "Train duplicates: 0\n",
      "Test duplicates: 0\n",
      "✓ No duplicates\n",
      "\n",
      "--- ID Structure ---\n",
      "Unique patients (subject_id): 16,317\n",
      "Unique admissions (hadm_id): 19,749\n",
      "Unique ICU stays (icustay_id): 20,885\n",
      "\n",
      "--- Visit Statistics ---\n",
      "Mean ICU stays per patient: 1.28\n",
      "Median: 1\n",
      "Max: 25\n",
      "Patients with multiple visits: 2,940 (18.0%)\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 1 COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1: LOAD DATA AND BASIC INSPECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "data_path = Path(\"../data/\")\n",
    "\n",
    "# Load main datasets\n",
    "train_raw = pd.read_csv(data_path / \"mimic_train_HEF.csv\", low_memory=False)\n",
    "test_raw = pd.read_csv(data_path / \"mimic_test_HEF.csv\", low_memory=False)\n",
    "\n",
    "# Load diagnoses\n",
    "diagnoses_raw = pd.read_csv(data_path / \"extra_data\" / \"MIMIC_diagnoses.csv\")\n",
    "\n",
    "print(f\"\\n✓ Loaded successfully:\")\n",
    "print(f\"  Train: {train_raw.shape}\")\n",
    "print(f\"  Test: {test_raw.shape}\")\n",
    "print(f\"  Diagnoses: {diagnoses_raw.shape}\")\n",
    "\n",
    "# Normalize column names (handle both upper/lower case)\n",
    "train = train_raw.copy()\n",
    "test = test_raw.copy()\n",
    "diagnoses = diagnoses_raw.copy()\n",
    "\n",
    "# Convert diagnoses columns to uppercase for consistency\n",
    "diagnoses.columns = diagnoses.columns.str.upper()\n",
    "\n",
    "print(f\"\\n✓ Column names standardized\")\n",
    "\n",
    "\n",
    "\n",
    "# Display key info\n",
    "print(f\"\\n--- Dataset Overview ---\")\n",
    "print(f\"Train samples: {len(train):,}\")\n",
    "print(f\"Test samples: {len(test):,}\")\n",
    "print(f\"Diagnosis records: {len(diagnoses):,}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\n--- Duplicate Check ---\")\n",
    "train_dupes = train['icustay_id'].duplicated().sum()\n",
    "test_dupes = test['icustay_id'].duplicated().sum()\n",
    "print(f\"Train duplicates: {train_dupes}\")\n",
    "print(f\"Test duplicates: {test_dupes}\")\n",
    "\n",
    "if train_dupes > 0 or test_dupes > 0:\n",
    "    print(\"⚠️ WARNING: Duplicates found!\")\n",
    "else:\n",
    "    print(\"✓ No duplicates\")\n",
    "\n",
    "# Check ID structure\n",
    "print(f\"\\n--- ID Structure ---\")\n",
    "print(f\"Unique patients (subject_id): {train['subject_id'].nunique():,}\")\n",
    "print(f\"Unique admissions (hadm_id): {train['hadm_id'].nunique():,}\")\n",
    "print(f\"Unique ICU stays (icustay_id): {train['icustay_id'].nunique():,}\")\n",
    "\n",
    "# Patient visit statistics\n",
    "visits_per_patient = train.groupby('subject_id').size()\n",
    "print(f\"\\n--- Visit Statistics ---\")\n",
    "print(f\"Mean ICU stays per patient: {visits_per_patient.mean():.2f}\")\n",
    "print(f\"Median: {visits_per_patient.median():.0f}\")\n",
    "print(f\"Max: {visits_per_patient.max():.0f}\")\n",
    "print(f\"Patients with multiple visits: {(visits_per_patient > 1).sum():,} ({(visits_per_patient > 1).sum()/len(visits_per_patient)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 1 COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b9524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 2: CREATING HOSPITAL HISTORY FEATURES\n",
      "================================================================================\n",
      "\n",
      "--- Processing train ---\n",
      "  ✓ Sorted by patient and admission time\n",
      "\n",
      "  Validation:\n",
      "    n_previous_icu_stays - Min: 0, Max: 24, Mean: 0.40\n",
      "    is_first_icu_visit - First visits: 16317 (78.1%)\n",
      "    is_frequent_flyer - Frequent flyers: 3388 (16.2%)\n",
      "    ✓ Check passed: All first visits correctly marked\n",
      "\n",
      "--- Processing test ---\n",
      "  ✓ Sorted by patient and admission time\n",
      "\n",
      "  Validation:\n",
      "    n_previous_icu_stays - Min: 0, Max: 4, Mean: 0.09\n",
      "    is_first_icu_visit - First visits: 4847 (92.8%)\n",
      "    is_frequent_flyer - Frequent flyers: 172 (3.3%)\n",
      "    ✓ Check passed: All first visits correctly marked\n",
      "\n",
      "✓ Saved 5221 test IDs in correct order\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 2 COMPLETE - Hospital history features created\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: HOSPITAL HISTORY FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: CREATING HOSPITAL HISTORY FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_hospital_history_features(df, df_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Create features based on patient's visit history\n",
    "    \n",
    "    Features created:\n",
    "    - n_previous_icu_stays: Number of previous ICU visits for this patient\n",
    "    - is_first_icu_visit: Binary flag for first-time ICU patients\n",
    "    - is_frequent_flyer: Binary flag for patients with 3+ visits\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {df_name} ---\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by patient and time\n",
    "    if 'ADMITTIME' in df.columns:\n",
    "        df['ADMITTIME'] = pd.to_datetime(df['ADMITTIME'], errors='coerce')\n",
    "        df = df.sort_values(['subject_id', 'ADMITTIME'])\n",
    "        print(\"  ✓ Sorted by patient and admission time\")\n",
    "    else:\n",
    "        df = df.sort_values(['subject_id', 'hadm_id', 'icustay_id'])\n",
    "        print(\"  ✓ Sorted by patient and IDs\")\n",
    "    \n",
    "    # Feature 1: Previous ICU stays\n",
    "    df['n_previous_icu_stays'] = df.groupby('subject_id').cumcount()\n",
    "    \n",
    "    # Feature 2: First visit flag\n",
    "    df['is_first_icu_visit'] = (df['n_previous_icu_stays'] == 0).astype(int)\n",
    "    \n",
    "    # Feature 3: Frequent flyer (3+ visits in entire dataset)\n",
    "    total_visits = df.groupby('subject_id').size()\n",
    "    frequent_patients = total_visits[total_visits >= 3].index\n",
    "    df['is_frequent_flyer'] = df['subject_id'].isin(frequent_patients).astype(int)\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\n  Validation:\")\n",
    "    print(f\"    n_previous_icu_stays - Min: {df['n_previous_icu_stays'].min()}, Max: {df['n_previous_icu_stays'].max()}, Mean: {df['n_previous_icu_stays'].mean():.2f}\")\n",
    "    print(f\"    is_first_icu_visit - First visits: {df['is_first_icu_visit'].sum()} ({df['is_first_icu_visit'].mean()*100:.1f}%)\")\n",
    "    print(f\"    is_frequent_flyer - Frequent flyers: {df['is_frequent_flyer'].sum()} ({df['is_frequent_flyer'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Check: Every patient's first row should have n_previous = 0\n",
    "    first_rows = df.groupby('subject_id').first()\n",
    "    assert (first_rows['n_previous_icu_stays'] == 0).all(), \"ERROR: Not all first visits have n_previous = 0!\"\n",
    "    print(f\"    ✓ Check passed: All first visits correctly marked\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to train and test\n",
    "train = create_hospital_history_features(train, \"train\")\n",
    "test = create_hospital_history_features(test, \"test\")\n",
    "\n",
    "# Save test IDs NOW (after sorting!)\n",
    "test_ids = test['icustay_id'].copy()\n",
    "print(f\"\\n✓ Saved {len(test_ids)} test IDs in correct order\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 2 COMPLETE - Hospital history features created\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0881292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 3: CREATING ICD9 DIAGNOSIS FEATURES\n",
      "================================================================================\n",
      "\n",
      "--- Diagnoses Data Structure ---\n",
      "Columns: ['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE']\n",
      "Sample:\n",
      "   SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
      "0         256   108811      1.0     53240\n",
      "1         256   108811      2.0     41071\n",
      "2         256   108811      3.0     53560\n",
      "\n",
      "--- Missing Values in Diagnoses ---\n",
      "SUBJECT_ID     0\n",
      "HADM_ID        0\n",
      "SEQ_NUM       47\n",
      "ICD9_CODE     47\n",
      "dtype: int64\n",
      "\n",
      "--- Building Diagnosis Features ---\n",
      "\n",
      "1. Number of diagnoses per admission:\n",
      "   Mean: 11.0\n",
      "   Median: 9\n",
      "   Max: 39\n",
      "   Train - Admissions with diagnoses: 20885 (100.0%)\n",
      "   Test - Admissions with diagnoses: 5221 (100.0%)\n",
      "\n",
      "2. Primary diagnoses:\n",
      "   Unique primary diagnoses: 2789\n",
      "   Train - Matched: 20885 (100.0%)\n",
      "   Test - Matched: 5221 (100.0%)\n",
      "\n",
      "3. ICD9 categories (3-digit):\n",
      "   Unique categories: 530\n",
      "   Top 5 categories:\n",
      "     038: 1595 (7.6%)\n",
      "     414: 1115 (5.3%)\n",
      "     410: 948 (4.5%)\n",
      "     424: 744 (3.6%)\n",
      "     428: 686 (3.3%)\n",
      "\n",
      "4. Major disease categories:\n",
      "   BLOOD: 7507 (35.9%)\n",
      "   MENTAL: 3912 (18.7%)\n",
      "   INFECTIOUS: 3208 (15.4%)\n",
      "   CIRCULATORY: 1795 (8.6%)\n",
      "   RESPIRATORY: 1578 (7.6%)\n",
      "   NEOPLASM: 1387 (6.6%)\n",
      "   NERVOUS: 810 (3.9%)\n",
      "   ENDOCRINE: 623 (3.0%)\n",
      "   V_CODE: 65 (0.3%)\n",
      "\n",
      "5. High-risk condition flags:\n",
      "   has_sepsis: 2805 (13.4%)\n",
      "   has_heart_failure: 5463 (26.2%)\n",
      "   has_respiratory_failure: 6116 (29.3%)\n",
      "   has_aki: 5842 (28.0%)\n",
      "   has_diabetes: 6125 (29.3%)\n",
      "   has_copd: 2749 (13.2%)\n",
      "   has_pneumonia: 3177 (15.2%)\n",
      "\n",
      "--- Validation Checks ---\n",
      "  ✓ Check passed: n_diagnoses consistent with primary diagnosis\n",
      "  ✓ Condition flag validation complete\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 3 COMPLETE - ICD9 features created and validated\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: ICD9 DIAGNOSIS FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: CREATING ICD9 DIAGNOSIS FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify diagnoses data structure\n",
    "print(f\"\\n--- Diagnoses Data Structure ---\")\n",
    "print(f\"Columns: {diagnoses.columns.tolist()}\")\n",
    "print(f\"Sample:\")\n",
    "print(diagnoses.head(3))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n--- Missing Values in Diagnoses ---\")\n",
    "print(diagnoses.isnull().sum())\n",
    "\n",
    "# Build diagnosis lookup\n",
    "print(f\"\\n--- Building Diagnosis Features ---\")\n",
    "\n",
    "# Feature 1: Number of diagnoses per admission\n",
    "n_diagnoses_per_admission = diagnoses.groupby('HADM_ID').size()\n",
    "print(f\"\\n1. Number of diagnoses per admission:\")\n",
    "print(f\"   Mean: {n_diagnoses_per_admission.mean():.1f}\")\n",
    "print(f\"   Median: {n_diagnoses_per_admission.median():.0f}\")\n",
    "print(f\"   Max: {n_diagnoses_per_admission.max():.0f}\")\n",
    "\n",
    "train['n_diagnoses'] = train['hadm_id'].map(n_diagnoses_per_admission).fillna(0).astype(int)\n",
    "test['n_diagnoses'] = test['hadm_id'].map(n_diagnoses_per_admission).fillna(0).astype(int)\n",
    "\n",
    "print(f\"   Train - Admissions with diagnoses: {(train['n_diagnoses'] > 0).sum()} ({(train['n_diagnoses'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"   Test - Admissions with diagnoses: {(test['n_diagnoses'] > 0).sum()} ({(test['n_diagnoses'] > 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Feature 2: Primary diagnosis (SEQ_NUM = 1)\n",
    "primary_diagnoses = diagnoses[diagnoses['SEQ_NUM'] == 1][['HADM_ID', 'ICD9_CODE']].set_index('HADM_ID')['ICD9_CODE']\n",
    "print(f\"\\n2. Primary diagnoses:\")\n",
    "print(f\"   Unique primary diagnoses: {primary_diagnoses.nunique()}\")\n",
    "\n",
    "train['primary_diagnosis_raw'] = train['hadm_id'].map(primary_diagnoses)\n",
    "test['primary_diagnosis_raw'] = test['hadm_id'].map(primary_diagnoses)\n",
    "\n",
    "print(f\"   Train - Matched: {train['primary_diagnosis_raw'].notna().sum()} ({train['primary_diagnosis_raw'].notna().mean()*100:.1f}%)\")\n",
    "print(f\"   Test - Matched: {test['primary_diagnosis_raw'].notna().sum()} ({test['primary_diagnosis_raw'].notna().mean()*100:.1f}%)\")\n",
    "\n",
    "# Feature 3: ICD9 category (first 3 characters)\n",
    "def extract_icd9_category(code):\n",
    "    \"\"\"Extract first 3 characters from ICD9 code\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return 'UNKNOWN'\n",
    "    code_str = str(code).strip().replace('.', '').replace(' ', '')\n",
    "    if len(code_str) >= 3:\n",
    "        return code_str[:3]\n",
    "    elif len(code_str) > 0:\n",
    "        return code_str\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "train['primary_diag_cat'] = train['primary_diagnosis_raw'].apply(extract_icd9_category)\n",
    "test['primary_diag_cat'] = test['primary_diagnosis_raw'].apply(extract_icd9_category)\n",
    "\n",
    "print(f\"\\n3. ICD9 categories (3-digit):\")\n",
    "print(f\"   Unique categories: {train['primary_diag_cat'].nunique()}\")\n",
    "print(f\"   Top 5 categories:\")\n",
    "for cat, count in train['primary_diag_cat'].value_counts().head().items():\n",
    "    print(f\"     {cat}: {count} ({count/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Feature 4: Major disease category (first digit)\n",
    "def get_disease_category(code):\n",
    "    \"\"\"Map ICD9 code to major disease category\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    code_str = str(code).strip().replace('.', '').replace(' ', '')\n",
    "    if len(code_str) == 0:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    first_char = code_str[0].upper()\n",
    "    \n",
    "    # ICD9 structure\n",
    "    if first_char in ['0', '1']:\n",
    "        return 'INFECTIOUS'\n",
    "    elif first_char == '2':\n",
    "        return 'NEOPLASM'\n",
    "    elif first_char == '3':\n",
    "        return 'ENDOCRINE'\n",
    "    elif first_char == '4':\n",
    "        return 'BLOOD'\n",
    "    elif first_char == '5':\n",
    "        return 'MENTAL'\n",
    "    elif first_char in ['6', '7']:\n",
    "        return 'NERVOUS'\n",
    "    elif first_char == '8':\n",
    "        return 'CIRCULATORY'\n",
    "    elif first_char == '9':\n",
    "        return 'RESPIRATORY'\n",
    "    elif first_char == 'V':\n",
    "        return 'V_CODE'  # Supplementary classification\n",
    "    elif first_char == 'E':\n",
    "        return 'E_CODE'  # External causes\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "train['disease_category'] = train['primary_diagnosis_raw'].apply(get_disease_category)\n",
    "test['disease_category'] = test['primary_diagnosis_raw'].apply(get_disease_category)\n",
    "\n",
    "print(f\"\\n4. Major disease categories:\")\n",
    "for cat, count in train['disease_category'].value_counts().items():\n",
    "    print(f\"   {cat}: {count} ({count/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Feature 5: Specific high-risk condition flags\n",
    "print(f\"\\n5. High-risk condition flags:\")\n",
    "\n",
    "# Build efficient lookup: hadm_id -> set of all ICD9 codes\n",
    "all_diagnoses_per_admission = diagnoses.groupby('HADM_ID')['ICD9_CODE'].apply(\n",
    "    lambda x: set(str(code).replace('.', '').replace(' ', '') for code in x)\n",
    ")\n",
    "\n",
    "def check_condition_presence(hadm_id, code_patterns):\n",
    "    \"\"\"Check if any diagnosis matches the pattern\"\"\"\n",
    "    if hadm_id not in all_diagnoses_per_admission.index:\n",
    "        return 0\n",
    "    \n",
    "    codes = all_diagnoses_per_admission[hadm_id]\n",
    "    \n",
    "    for pattern in code_patterns:\n",
    "        if any(code.startswith(pattern) for code in codes):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Define condition patterns\n",
    "conditions = {\n",
    "    'has_sepsis': ['99591', '99592', '78552'],  # Sepsis codes\n",
    "    'has_heart_failure': ['428'],  # Heart failure\n",
    "    'has_respiratory_failure': ['518'],  # Respiratory failure\n",
    "    'has_aki': ['584'],  # Acute kidney injury\n",
    "    'has_diabetes': ['250'],  # Diabetes\n",
    "    'has_copd': ['491', '492', '496'],  # COPD\n",
    "    'has_pneumonia': ['480', '481', '482', '483', '484', '485', '486']  # Pneumonia\n",
    "}\n",
    "\n",
    "for condition_name, patterns in conditions.items():\n",
    "    train[condition_name] = train['hadm_id'].apply(lambda x: check_condition_presence(x, patterns))\n",
    "    test[condition_name] = test['hadm_id'].apply(lambda x: check_condition_presence(x, patterns))\n",
    "    \n",
    "    count = train[condition_name].sum()\n",
    "    print(f\"   {condition_name}: {count} ({count/len(train)*100:.1f}%)\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"\\n--- Validation Checks ---\")\n",
    "\n",
    "# Check 1: n_diagnoses should be >= 1 if we have a primary diagnosis\n",
    "has_primary = train['primary_diagnosis_raw'].notna()\n",
    "has_n_diag = train['n_diagnoses'] > 0\n",
    "mismatch = has_primary & ~has_n_diag\n",
    "if mismatch.sum() > 0:\n",
    "    print(f\"  ⚠️ Warning: {mismatch.sum()} cases have primary diagnosis but n_diagnoses=0\")\n",
    "else:\n",
    "    print(f\"  ✓ Check passed: n_diagnoses consistent with primary diagnosis\")\n",
    "\n",
    "# Check 2: Condition flags should be <= n_diagnoses\n",
    "condition_cols = [col for col in train.columns if col.startswith('has_')]\n",
    "for col in condition_cols:\n",
    "    # If has_condition=1, should have n_diagnoses >= 1\n",
    "    invalid = (train[col] == 1) & (train['n_diagnoses'] == 0)\n",
    "    if invalid.sum() > 0:\n",
    "        print(f\"  ⚠️ Warning: {invalid.sum()} cases with {col}=1 but n_diagnoses=0\")\n",
    "\n",
    "print(f\"  ✓ Condition flag validation complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 3 COMPLETE - ICD9 features created and validated\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b86d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 4: REMOVING LEAKAGE COLUMNS\n",
      "================================================================================\n",
      "\n",
      "--- Columns to Drop ---\n",
      "  DISCHTIME                 Train:✓  Test:✗\n",
      "  DEATHTIME                 Train:✓  Test:✗\n",
      "  DOD                       Train:✓  Test:✗\n",
      "  LOS                       Train:✓  Test:✗\n",
      "  Diff                      Train:✓  Test:✓\n",
      "  ADMITTIME                 Train:✓  Test:✓\n",
      "  icustay_id                Train:✓  Test:✓\n",
      "  subject_id                Train:✓  Test:✓\n",
      "  hadm_id                   Train:✓  Test:✓\n",
      "  primary_diagnosis_raw     Train:✓  Test:✓\n",
      "\n",
      "--- Shape Changes ---\n",
      "  Train: (20885, 58) → (20885, 48)\n",
      "  Test:  (5221, 53) → (5221, 47)\n",
      "\n",
      "--- Separating Target ---\n",
      "  ✓ Target separated\n",
      "  ✓ y shape: (20885,)\n",
      "  ✓ X shape: (20885, 47)\n",
      "  ✓ X_test shape: (5221, 47)\n",
      "\n",
      "--- Target Validation ---\n",
      "  Target name: HOSPITAL_EXPIRE_FLAG\n",
      "  Unique values: [0 1]\n",
      "  Mortality rate: 0.112 (2345/20885)\n",
      "  Class balance: 0=18540, 1=2345\n",
      "  ✓ Mortality rate matches expected (~11.2%)\n",
      "\n",
      "--- Column Consistency Check ---\n",
      "  ✓ Train and test have identical columns (47 columns)\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 4 COMPLETE - Leakage columns removed, target separated\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: DROP LEAKAGE COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: REMOVING LEAKAGE COLUMNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Columns that leak information about the target\n",
    "leakage_columns = [\n",
    "    'DISCHTIME',      # Discharge time (only known after outcome)\n",
    "    'DEATHTIME',      # Death time (IS the target!)\n",
    "    'DOD',            # Date of death (IS the target!)\n",
    "    'LOS',            # Length of stay (correlated with outcome)\n",
    "    'Diff',           # Some time difference (likely leakage)\n",
    "    'ADMITTIME',      # We already used it for history features, now drop\n",
    "]\n",
    "\n",
    "# IDs - we've extracted all useful info, now drop\n",
    "id_columns = [\n",
    "    'icustay_id',     # Already saved as test_ids\n",
    "    'subject_id',     # Used for history features, now drop\n",
    "    'hadm_id',        # Used for diagnosis matching, now drop\n",
    "]\n",
    "\n",
    "# Diagnosis column - we've extracted all features from it\n",
    "diagnosis_columns = [\n",
    "    'primary_diagnosis_raw'  # Keep the encoded versions only\n",
    "]\n",
    "\n",
    "all_columns_to_drop = leakage_columns + id_columns + diagnosis_columns\n",
    "\n",
    "print(f\"\\n--- Columns to Drop ---\")\n",
    "for col in all_columns_to_drop:\n",
    "    train_has = \"✓\" if col in train.columns else \"✗\"\n",
    "    test_has = \"✓\" if col in test.columns else \"✗\"\n",
    "    print(f\"  {col:25s} Train:{train_has}  Test:{test_has}\")\n",
    "\n",
    "# Drop from train\n",
    "train_clean = train.drop(columns=[c for c in all_columns_to_drop if c in train.columns], errors='ignore')\n",
    "\n",
    "# Drop from test  \n",
    "test_clean = test.drop(columns=[c for c in all_columns_to_drop if c in test.columns], errors='ignore')\n",
    "\n",
    "print(f\"\\n--- Shape Changes ---\")\n",
    "print(f\"  Train: {train.shape} → {train_clean.shape}\")\n",
    "print(f\"  Test:  {test.shape} → {test_clean.shape}\")\n",
    "\n",
    "# Separate target from train\n",
    "print(f\"\\n--- Separating Target ---\")\n",
    "if 'HOSPITAL_EXPIRE_FLAG' not in train_clean.columns:\n",
    "    print(\"  ❌ ERROR: Target column not found!\")\n",
    "    raise ValueError(\"HOSPITAL_EXPIRE_FLAG column missing!\")\n",
    "\n",
    "y = train_clean['HOSPITAL_EXPIRE_FLAG'].copy()\n",
    "X = train_clean.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "X_test = test_clean.copy()\n",
    "\n",
    "print(f\"  ✓ Target separated\")\n",
    "print(f\"  ✓ y shape: {y.shape}\")\n",
    "print(f\"  ✓ X shape: {X.shape}\")\n",
    "print(f\"  ✓ X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Validate target\n",
    "print(f\"\\n--- Target Validation ---\")\n",
    "print(f\"  Target name: HOSPITAL_EXPIRE_FLAG\")\n",
    "print(f\"  Unique values: {y.unique()}\")\n",
    "print(f\"  Mortality rate: {y.mean():.3f} ({y.sum()}/{len(y)})\")\n",
    "print(f\"  Class balance: 0={y.value_counts()[0]}, 1={y.value_counts()[1]}\")\n",
    "\n",
    "expected_mortality = 0.112\n",
    "if abs(y.mean() - expected_mortality) > 0.01:\n",
    "    print(f\"  ⚠️ Warning: Mortality rate {y.mean():.3f} differs from expected {expected_mortality:.3f}\")\n",
    "else:\n",
    "    print(f\"  ✓ Mortality rate matches expected (~11.2%)\")\n",
    "\n",
    "# Verify train and test have same columns (except target)\n",
    "print(f\"\\n--- Column Consistency Check ---\")\n",
    "train_cols = set(X.columns)\n",
    "test_cols = set(X_test.columns)\n",
    "\n",
    "cols_only_in_train = train_cols - test_cols\n",
    "cols_only_in_test = test_cols - train_cols\n",
    "\n",
    "if cols_only_in_train:\n",
    "    print(f\"  ⚠️ Columns only in train: {cols_only_in_train}\")\n",
    "if cols_only_in_test:\n",
    "    print(f\"  ⚠️ Columns only in test: {cols_only_in_test}\")\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(f\"  ✓ Train and test have identical columns ({len(train_cols)} columns)\")\n",
    "else:\n",
    "    print(f\"  ❌ ERROR: Train and test column mismatch!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 4 COMPLETE - Leakage columns removed, target separated\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4cbdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 5: CONVERTING DOB TO AGE\n",
      "================================================================================\n",
      "\n",
      "--- Loading original data for ADMITTIME ---\n",
      "  ✓ Original data loaded\n",
      "\n",
      "--- Converting dates ---\n",
      "  Train - DOB parsed: 20885/20885\n",
      "  Train - ADMITTIME parsed: 20885/20885\n",
      "  Test - DOB parsed: 5221/5221\n",
      "  Test - ADMITTIME parsed: 5221/5221\n",
      "\n",
      "--- Calculating ages ---\n",
      "  ✓ Ages calculated\n",
      "\n",
      "--- Age Distribution (Before Cleaning) ---\n",
      "  Train:\n",
      "    Min: -71.9\n",
      "    Max: 292.0\n",
      "    Mean: 67.4\n",
      "    Median: 64.6\n",
      "    Missing: 635\n",
      "\n",
      "--- Cleaning Invalid Ages ---\n",
      "  Train - Invalid ages: 4340\n",
      "  Test - Invalid ages: 1063\n",
      "    Sample invalid ages: [242.56810403832992, -11.915126625598905, -11.118412046543463, -15.780971937029431, -13.05407255304586]\n",
      "  ✓ Imputed 4975 train + 1222 test missing ages with median: 62.0\n",
      "\n",
      "--- Age Distribution (After Cleaning) ---\n",
      "  Train:\n",
      "    Range: 0.0 - 120.0 years\n",
      "    Mean: 61.4 years\n",
      "    Std: 27.3 years\n",
      "    25th percentile: 44.9\n",
      "    50th percentile: 62.0\n",
      "    75th percentile: 78.2\n",
      "\n",
      "  ✓ Dropped DOB column\n",
      "\n",
      "--- Validation ---\n",
      "  ✓ All validation checks passed\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 5 COMPLETE - DOB converted to age\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: CONVERT DOB TO AGE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: CONVERTING DOB TO AGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'DOB' not in X.columns:\n",
    "    print(\"  ⚠️ DOB column not found, skipping age calculation\")\n",
    "else:\n",
    "    print(\"\\n--- Loading original data for ADMITTIME ---\")\n",
    "    \n",
    "    # Need to reload to get ADMITTIME (we dropped it earlier)\n",
    "    train_original = pd.read_csv(data_path / 'mimic_train_HEF.csv')\n",
    "    test_original = pd.read_csv(data_path / 'mimic_test_HEF.csv')\n",
    "    \n",
    "    print(\"  ✓ Original data loaded\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    print(\"\\n--- Converting dates ---\")\n",
    "    dob_train = pd.to_datetime(X['DOB'], errors='coerce')\n",
    "    dob_test = pd.to_datetime(X_test['DOB'], errors='coerce')\n",
    "    admit_train = pd.to_datetime(train_original['ADMITTIME'], errors='coerce')\n",
    "    admit_test = pd.to_datetime(test_original['ADMITTIME'], errors='coerce')\n",
    "    \n",
    "    print(f\"  Train - DOB parsed: {dob_train.notna().sum()}/{len(dob_train)}\")\n",
    "    print(f\"  Train - ADMITTIME parsed: {admit_train.notna().sum()}/{len(admit_train)}\")\n",
    "    print(f\"  Test - DOB parsed: {dob_test.notna().sum()}/{len(dob_test)}\")\n",
    "    print(f\"  Test - ADMITTIME parsed: {admit_test.notna().sum()}/{len(admit_test)}\")\n",
    "    \n",
    "    # Calculate age\n",
    "    print(\"\\n--- Calculating ages ---\")\n",
    "    \n",
    "    def calculate_age(admit_time, dob):\n",
    "        \"\"\"Calculate age in years from admission time and DOB\"\"\"\n",
    "        if pd.isna(admit_time) or pd.isna(dob):\n",
    "            return np.nan\n",
    "        try:\n",
    "            age_days = (admit_time - dob).days\n",
    "            age_years = age_days / 365.25\n",
    "            return age_years\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    X['age'] = [calculate_age(admit, dob) for admit, dob in zip(admit_train, dob_train)]\n",
    "    X_test['age'] = [calculate_age(admit, dob) for admit, dob in zip(admit_test, dob_test)]\n",
    "    \n",
    "    # Convert to numeric\n",
    "    X['age'] = pd.to_numeric(X['age'], errors='coerce')\n",
    "    X_test['age'] = pd.to_numeric(X_test['age'], errors='coerce')\n",
    "    \n",
    "    print(f\"  ✓ Ages calculated\")\n",
    "    \n",
    "    # Analyze age distribution\n",
    "    print(f\"\\n--- Age Distribution (Before Cleaning) ---\")\n",
    "    print(f\"  Train:\")\n",
    "    print(f\"    Min: {X['age'].min():.1f}\")\n",
    "    print(f\"    Max: {X['age'].max():.1f}\")\n",
    "    print(f\"    Mean: {X['age'].mean():.1f}\")\n",
    "    print(f\"    Median: {X['age'].median():.1f}\")\n",
    "    print(f\"    Missing: {X['age'].isna().sum()}\")\n",
    "    \n",
    "    # Clean invalid ages\n",
    "    print(f\"\\n--- Cleaning Invalid Ages ---\")\n",
    "    \n",
    "    # Flag invalid ages (< 0 or > 120)\n",
    "    invalid_train = (X['age'] < 0) | (X['age'] > 120)\n",
    "    invalid_test = (X_test['age'] < 0) | (X_test['age'] > 120)\n",
    "    \n",
    "    print(f\"  Train - Invalid ages: {invalid_train.sum()}\")\n",
    "    print(f\"  Test - Invalid ages: {invalid_test.sum()}\")\n",
    "    \n",
    "    if invalid_train.sum() > 0:\n",
    "        print(f\"    Sample invalid ages: {X.loc[invalid_train, 'age'].head().tolist()}\")\n",
    "    \n",
    "    # Set invalid to NaN\n",
    "    X.loc[invalid_train, 'age'] = np.nan\n",
    "    X_test.loc[invalid_test, 'age'] = np.nan\n",
    "    \n",
    "    # Impute missing ages with median\n",
    "    age_median = X['age'].median()\n",
    "    n_missing_train = X['age'].isna().sum()\n",
    "    n_missing_test = X_test['age'].isna().sum()\n",
    "    \n",
    "    X['age'].fillna(age_median, inplace=True)\n",
    "    X_test['age'].fillna(age_median, inplace=True)\n",
    "    \n",
    "    print(f\"  ✓ Imputed {n_missing_train} train + {n_missing_test} test missing ages with median: {age_median:.1f}\")\n",
    "    \n",
    "    # Final age distribution\n",
    "    print(f\"\\n--- Age Distribution (After Cleaning) ---\")\n",
    "    print(f\"  Train:\")\n",
    "    print(f\"    Range: {X['age'].min():.1f} - {X['age'].max():.1f} years\")\n",
    "    print(f\"    Mean: {X['age'].mean():.1f} years\")\n",
    "    print(f\"    Std: {X['age'].std():.1f} years\")\n",
    "    \n",
    "    # Age percentiles\n",
    "    percentiles = X['age'].quantile([0.25, 0.5, 0.75])\n",
    "    print(f\"    25th percentile: {percentiles[0.25]:.1f}\")\n",
    "    print(f\"    50th percentile: {percentiles[0.5]:.1f}\")\n",
    "    print(f\"    75th percentile: {percentiles[0.75]:.1f}\")\n",
    "    \n",
    "    # Drop DOB column\n",
    "    X = X.drop('DOB', axis=1)\n",
    "    X_test = X_test.drop('DOB', axis=1)\n",
    "    \n",
    "    print(f\"\\n  ✓ Dropped DOB column\")\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\n--- Validation ---\")\n",
    "    assert X['age'].notna().all(), \"ERROR: Still have NaN ages in train!\"\n",
    "    assert X_test['age'].notna().all(), \"ERROR: Still have NaN ages in test!\"\n",
    "    assert (X['age'] >= 0).all() and (X['age'] <= 120).all(), \"ERROR: Invalid ages in train!\"\n",
    "    assert (X_test['age'] >= 0).all() and (X_test['age'] <= 120).all(), \"ERROR: Invalid ages in test!\"\n",
    "    \n",
    "    print(f\"  ✓ All validation checks passed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 5 COMPLETE - DOB converted to age\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01852d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 6: FEATURE TYPE IDENTIFICATION & IMPUTATION\n",
      "================================================================================\n",
      "\n",
      "--- Identifying Feature Types ---\n",
      "  Numeric features: 36\n",
      "  Categorical features: 11\n",
      "\n",
      "--- Sample Features ---\n",
      "  Numeric (first 10): ['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min', 'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean', 'MeanBP_Min']\n",
      "  Categorical: ['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'DIAGNOSIS', 'ICD9_diagnosis', 'FIRST_CAREUNIT', 'primary_diag_cat', 'disease_category']\n",
      "\n",
      "--- Missing Value Analysis ---\n",
      "  Numeric features with missing values:\n",
      "    TempC_Max                        2497 ( 12.0%)\n",
      "    TempC_Mean                       2497 ( 12.0%)\n",
      "    TempC_Min                        2497 ( 12.0%)\n",
      "    DiasBP_Min                       2209 ( 10.6%)\n",
      "    DiasBP_Mean                      2209 ( 10.6%)\n",
      "    DiasBP_Max                       2209 ( 10.6%)\n",
      "    SysBP_Min                        2208 ( 10.6%)\n",
      "    SysBP_Mean                       2208 ( 10.6%)\n",
      "    SysBP_Max                        2208 ( 10.6%)\n",
      "    SpO2_Max                         2203 ( 10.5%)\n",
      "    SpO2_Mean                        2203 ( 10.5%)\n",
      "    SpO2_Min                         2203 ( 10.5%)\n",
      "    RespRate_Max                     2189 ( 10.5%)\n",
      "    RespRate_Mean                    2189 ( 10.5%)\n",
      "    RespRate_Min                     2189 ( 10.5%)\n",
      "    HeartRate_Mean                   2187 ( 10.5%)\n",
      "    HeartRate_Max                    2187 ( 10.5%)\n",
      "    HeartRate_Min                    2187 ( 10.5%)\n",
      "    MeanBP_Mean                      2186 ( 10.5%)\n",
      "    MeanBP_Min                       2186 ( 10.5%)\n",
      "    MeanBP_Max                       2186 ( 10.5%)\n",
      "    Glucose_Min                       253 (  1.2%)\n",
      "    Glucose_Max                       253 (  1.2%)\n",
      "    Glucose_Mean                      253 (  1.2%)\n",
      "\n",
      "  Categorical features with missing values:\n",
      "    MARITAL_STATUS                    722 (  3.5%)\n",
      "\n",
      "--- Imputation Strategy ---\n",
      "  Numeric features: Median imputation\n",
      "  ✓ Imputed 36 numeric features\n",
      "  Categorical features: Most frequent imputation\n",
      "  ✓ Imputed 11 categorical features\n",
      "\n",
      "--- Post-Imputation Validation ---\n",
      "  Train missing values: 0\n",
      "  Test missing values: 0\n",
      "  ✓ No missing values remain\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 6 COMPLETE - Features identified and imputed\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: IDENTIFY FEATURE TYPES AND HANDLE MISSING VALUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: FEATURE TYPE IDENTIFICATION & IMPUTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify feature types\n",
    "print(\"\\n--- Identifying Feature Types ---\")\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"  Numeric features: {len(numeric_features)}\")\n",
    "print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Show sample of each type\n",
    "print(f\"\\n--- Sample Features ---\")\n",
    "print(f\"  Numeric (first 10): {numeric_features[:10]}\")\n",
    "print(f\"  Categorical: {categorical_features}\")\n",
    "\n",
    "# Check missing values\n",
    "print(f\"\\n--- Missing Value Analysis ---\")\n",
    "\n",
    "missing_numeric = X[numeric_features].isnull().sum()\n",
    "missing_numeric = missing_numeric[missing_numeric > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_numeric) > 0:\n",
    "    print(f\"  Numeric features with missing values:\")\n",
    "    for feat, count in missing_numeric.items():\n",
    "        pct = count / len(X) * 100\n",
    "        print(f\"    {feat:30s} {count:6d} ({pct:5.1f}%)\")\n",
    "else:\n",
    "    print(f\"  ✓ No missing values in numeric features\")\n",
    "\n",
    "missing_categorical = X[categorical_features].isnull().sum()\n",
    "missing_categorical = missing_categorical[missing_categorical > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_categorical) > 0:\n",
    "    print(f\"\\n  Categorical features with missing values:\")\n",
    "    for feat, count in missing_categorical.items():\n",
    "        pct = count / len(X) * 100\n",
    "        print(f\"    {feat:30s} {count:6d} ({pct:5.1f}%)\")\n",
    "else:\n",
    "    print(f\"  ✓ No missing values in categorical features\")\n",
    "\n",
    "# Imputation\n",
    "print(f\"\\n--- Imputation Strategy ---\")\n",
    "\n",
    "# Numeric: median imputation\n",
    "if len(numeric_features) > 0:\n",
    "    print(f\"  Numeric features: Median imputation\")\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])\n",
    "    X_test[numeric_features] = numeric_imputer.transform(X_test[numeric_features])\n",
    "    print(f\"  ✓ Imputed {len(numeric_features)} numeric features\")\n",
    "\n",
    "# Categorical: most frequent imputation\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\"  Categorical features: Most frequent imputation\")\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
    "    X_test[categorical_features] = categorical_imputer.transform(X_test[categorical_features])\n",
    "    print(f\"  ✓ Imputed {len(categorical_features)} categorical features\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\n--- Post-Imputation Validation ---\")\n",
    "\n",
    "train_missing = X.isnull().sum().sum()\n",
    "test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"  Train missing values: {train_missing}\")\n",
    "print(f\"  Test missing values: {test_missing}\")\n",
    "\n",
    "if train_missing > 0 or test_missing > 0:\n",
    "    print(f\"  ❌ ERROR: Still have missing values after imputation!\")\n",
    "    if train_missing > 0:\n",
    "        print(f\"    Train columns with NaN: {X.columns[X.isnull().any()].tolist()}\")\n",
    "    if test_missing > 0:\n",
    "        print(f\"    Test columns with NaN: {X_test.columns[X_test.isnull().any()].tolist()}\")\n",
    "else:\n",
    "    print(f\"  ✓ No missing values remain\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 6 COMPLETE - Features identified and imputed\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55e0b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 7: ENCODING CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "--- Categorical Feature Analysis ---\n",
      "  GENDER                       2 unique values\n",
      "    Distribution:\n",
      "      M                               11759 ( 56.3%)\n",
      "      F                                9126 ( 43.7%)\n",
      "  ADMISSION_TYPE               3 unique values\n",
      "    Distribution:\n",
      "      EMERGENCY                       17817 ( 85.3%)\n",
      "      ELECTIVE                         2848 ( 13.6%)\n",
      "      URGENT                            220 (  1.1%)\n",
      "  INSURANCE                    5 unique values\n",
      "    Distribution:\n",
      "      Medicare                        11718 ( 56.1%)\n",
      "      Private                          6245 ( 29.9%)\n",
      "      Medicaid                         2117 ( 10.1%)\n",
      "      Government                        611 (  2.9%)\n",
      "      Self Pay                          194 (  0.9%)\n",
      "  RELIGION                    17 unique values\n",
      "  MARITAL_STATUS               7 unique values\n",
      "    Distribution:\n",
      "      MARRIED                         10386 ( 49.7%)\n",
      "      SINGLE                           5910 ( 28.3%)\n",
      "      WIDOWED                          2819 ( 13.5%)\n",
      "      DIVORCED                         1413 (  6.8%)\n",
      "      SEPARATED                         240 (  1.1%)\n",
      "  ETHNICITY                   41 unique values\n",
      "  DIAGNOSIS                 6193 unique values\n",
      "  ICD9_diagnosis            1853 unique values\n",
      "  FIRST_CAREUNIT               5 unique values\n",
      "    Distribution:\n",
      "      MICU                             8640 ( 41.4%)\n",
      "      SICU                             3961 ( 19.0%)\n",
      "      CSRU                             3127 ( 15.0%)\n",
      "      TSICU                            2645 ( 12.7%)\n",
      "      CCU                              2512 ( 12.0%)\n",
      "  primary_diag_cat           530 unique values\n",
      "  disease_category             9 unique values\n",
      "    Distribution:\n",
      "      BLOOD                            7507 ( 35.9%)\n",
      "      MENTAL                           3912 ( 18.7%)\n",
      "      INFECTIOUS                       3208 ( 15.4%)\n",
      "      CIRCULATORY                      1795 (  8.6%)\n",
      "      RESPIRATORY                      1578 (  7.6%)\n",
      "\n",
      "================================================================================\n",
      "ENCODING STRATEGY\n",
      "================================================================================\n",
      "\n",
      "1. ICD9_diagnosis → Target encode\n",
      "   ✓ Encoded 530 categories\n",
      "   Mortality range: 0.000 - 1.000\n",
      "\n",
      "2. primary_diag_cat → Target encode\n",
      "   ✓ Encoded 530 categories\n",
      "   Mortality range: 0.000 - 1.000\n",
      "\n",
      "3. DIAGNOSIS → Drop\n",
      "   Unique values: 6193\n",
      "   ✓ Dropping (free text, already have ICD9 codes)\n",
      "\n",
      "4. Grouping categorical features\n",
      "   ETHNICITY: 8 categories\n",
      "   RELIGION: 8 categories\n",
      "   MARITAL_STATUS: 5 categories\n",
      "\n",
      "5. One-hot encoding remaining features\n",
      "   Features to one-hot encode: ['GENDER', 'ADMISSION_TYPE', 'INSURANCE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY', 'FIRST_CAREUNIT', 'disease_category']\n",
      "   ✓ Created 37 binary features\n",
      "\n",
      "================================================================================\n",
      "ENCODING VALIDATION\n",
      "================================================================================\n",
      "  ✓ No object columns remain - all categorical features encoded\n",
      "  ✓ Train and test have identical columns: 75\n",
      "\n",
      "  Final feature count: 75\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 7 COMPLETE - Categorical features encoded\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: ENCODE CATEGORICAL FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7: ENCODING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Categorical Feature Analysis ---\")\n",
    "\n",
    "for cat_col in categorical_features:\n",
    "    n_unique = X[cat_col].nunique()\n",
    "    print(f\"  {cat_col:25s} {n_unique:4d} unique values\")\n",
    "    \n",
    "    # Show distribution for low-cardinality features\n",
    "    if n_unique <= 10:\n",
    "        print(f\"    Distribution:\")\n",
    "        for val, count in X[cat_col].value_counts().head(5).items():\n",
    "            print(f\"      {str(val):30s} {count:6d} ({count/len(X)*100:5.1f}%)\")\n",
    "\n",
    "# Strategy for each categorical feature\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- 1. ICD9_diagnosis: Target encode (already have primary_diag_cat) ---\n",
    "print(\"\\n1. ICD9_diagnosis → Target encode\")\n",
    "\n",
    "if 'ICD9_diagnosis' in X.columns:\n",
    "    def extract_icd9_category(code):\n",
    "        if pd.isna(code):\n",
    "            return 'UNKNOWN'\n",
    "        code_str = str(code).strip().replace('.', '')\n",
    "        if len(code_str) >= 3:\n",
    "            return code_str[:3]\n",
    "        elif len(code_str) > 0:\n",
    "            return code_str\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    X['ICD9_cat'] = X['ICD9_diagnosis'].apply(extract_icd9_category)\n",
    "    X_test['ICD9_cat'] = X_test['ICD9_diagnosis'].apply(extract_icd9_category)\n",
    "    \n",
    "    # Target encode\n",
    "    encoding_map = y.groupby(X['ICD9_cat']).mean().to_dict()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    X['ICD9_encoded'] = X['ICD9_cat'].map(encoding_map)\n",
    "    X_test['ICD9_encoded'] = X_test['ICD9_cat'].map(encoding_map).fillna(global_mean)\n",
    "    \n",
    "    print(f\"   ✓ Encoded {X['ICD9_cat'].nunique()} categories\")\n",
    "    print(f\"   Mortality range: {X['ICD9_encoded'].min():.3f} - {X['ICD9_encoded'].max():.3f}\")\n",
    "    \n",
    "    # Drop original columns\n",
    "    X = X.drop(['ICD9_diagnosis', 'ICD9_cat'], axis=1)\n",
    "    X_test = X_test.drop(['ICD9_diagnosis', 'ICD9_cat'], axis=1)\n",
    "    categorical_features.remove('ICD9_diagnosis')\n",
    "\n",
    "# --- 2. primary_diag_cat: Target encode ---\n",
    "print(\"\\n2. primary_diag_cat → Target encode\")\n",
    "\n",
    "if 'primary_diag_cat' in X.columns:\n",
    "    encoding_map = y.groupby(X['primary_diag_cat']).mean().to_dict()\n",
    "    \n",
    "    X['primary_diag_encoded'] = X['primary_diag_cat'].map(encoding_map)\n",
    "    X_test['primary_diag_encoded'] = X_test['primary_diag_cat'].map(encoding_map).fillna(global_mean)\n",
    "    \n",
    "    print(f\"   ✓ Encoded {X['primary_diag_cat'].nunique()} categories\")\n",
    "    print(f\"   Mortality range: {X['primary_diag_encoded'].min():.3f} - {X['primary_diag_encoded'].max():.3f}\")\n",
    "    \n",
    "    X = X.drop('primary_diag_cat', axis=1)\n",
    "    X_test = X_test.drop('primary_diag_cat', axis=1)\n",
    "    categorical_features.remove('primary_diag_cat')\n",
    "\n",
    "# --- 3. DIAGNOSIS: Drop (free text, too high cardinality) ---\n",
    "print(\"\\n3. DIAGNOSIS → Drop\")\n",
    "\n",
    "if 'DIAGNOSIS' in categorical_features:\n",
    "    print(f\"   Unique values: {X['DIAGNOSIS'].nunique()}\")\n",
    "    print(f\"   ✓ Dropping (free text, already have ICD9 codes)\")\n",
    "    \n",
    "    X = X.drop('DIAGNOSIS', axis=1)\n",
    "    X_test = X_test.drop('DIAGNOSIS', axis=1)\n",
    "    categorical_features.remove('DIAGNOSIS')\n",
    "\n",
    "# --- 4. Group low-frequency categories ---\n",
    "print(\"\\n4. Grouping categorical features\")\n",
    "\n",
    "# ETHNICITY\n",
    "if 'ETHNICITY' in categorical_features:\n",
    "    def group_ethnicity(ethnicity):\n",
    "        if pd.isna(ethnicity):\n",
    "            return 'UNKNOWN'\n",
    "        ethnicity = str(ethnicity).upper()\n",
    "        if 'WHITE' in ethnicity:\n",
    "            return 'WHITE'\n",
    "        elif 'BLACK' in ethnicity or 'AFRICAN' in ethnicity:\n",
    "            return 'BLACK'\n",
    "        elif 'HISPANIC' in ethnicity or 'LATINO' in ethnicity:\n",
    "            return 'HISPANIC'\n",
    "        elif 'ASIAN' in ethnicity:\n",
    "            return 'ASIAN'\n",
    "        elif 'AMERICAN INDIAN' in ethnicity or 'ALASKA NATIVE' in ethnicity:\n",
    "            return 'NATIVE'\n",
    "        elif 'HAWAIIAN' in ethnicity or 'PACIFIC ISLANDER' in ethnicity:\n",
    "            return 'PACIFIC_ISLANDER'\n",
    "        elif any(x in ethnicity for x in ['UNKNOWN', 'UNABLE', 'DECLINED', 'NOT SPECIFIED']):\n",
    "            return 'UNKNOWN'\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    X['ETHNICITY'] = X['ETHNICITY'].apply(group_ethnicity)\n",
    "    X_test['ETHNICITY'] = X_test['ETHNICITY'].apply(group_ethnicity)\n",
    "    \n",
    "    print(f\"   ETHNICITY: {X['ETHNICITY'].nunique()} categories\")\n",
    "\n",
    "# RELIGION\n",
    "if 'RELIGION' in categorical_features:\n",
    "    def group_religion(religion):\n",
    "        if pd.isna(religion):\n",
    "            return 'UNKNOWN'\n",
    "        religion = str(religion).upper()\n",
    "        if 'CATHOLIC' in religion:\n",
    "            return 'CATHOLIC'\n",
    "        elif any(x in religion for x in ['PROTESTANT', 'EPISCOPALIAN', 'QUAKER']):\n",
    "            return 'PROTESTANT'\n",
    "        elif 'JEWISH' in religion or 'HEBREW' in religion:\n",
    "            return 'JEWISH'\n",
    "        elif 'MUSLIM' in religion:\n",
    "            return 'MUSLIM'\n",
    "        elif 'ORTHODOX' in religion:\n",
    "            return 'ORTHODOX'\n",
    "        elif any(x in religion for x in ['BUDDHIST', 'HINDU', 'JEHOVAH', 'CHRISTIAN SCIENTIST']):\n",
    "            return 'OTHER_RELIGION'\n",
    "        elif any(x in religion for x in ['UNOBTAINABLE', 'NOT SPECIFIED', 'UNKNOWN']):\n",
    "            return 'UNKNOWN'\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    X['RELIGION'] = X['RELIGION'].apply(group_religion)\n",
    "    X_test['RELIGION'] = X_test['RELIGION'].apply(group_religion)\n",
    "    \n",
    "    print(f\"   RELIGION: {X['RELIGION'].nunique()} categories\")\n",
    "\n",
    "# MARITAL_STATUS\n",
    "if 'MARITAL_STATUS' in categorical_features:\n",
    "    def group_marital_status(status):\n",
    "        if pd.isna(status):\n",
    "            return 'UNKNOWN'\n",
    "        status = str(status).upper()\n",
    "        if 'MARRIED' in status or 'LIFE PARTNER' in status:\n",
    "            return 'MARRIED'\n",
    "        elif 'SINGLE' in status:\n",
    "            return 'SINGLE'\n",
    "        elif 'WIDOWED' in status:\n",
    "            return 'WIDOWED'\n",
    "        elif 'DIVORCED' in status or 'SEPARATED' in status:\n",
    "            return 'DIVORCED_SEPARATED'\n",
    "        else:\n",
    "            return 'UNKNOWN'\n",
    "    \n",
    "    X['MARITAL_STATUS'] = X['MARITAL_STATUS'].apply(group_marital_status)\n",
    "    X_test['MARITAL_STATUS'] = X_test['MARITAL_STATUS'].apply(group_marital_status)\n",
    "    \n",
    "    print(f\"   MARITAL_STATUS: {X['MARITAL_STATUS'].nunique()} categories\")\n",
    "\n",
    "# --- 5. One-hot encode remaining categoricals ---\n",
    "print(\"\\n5. One-hot encoding remaining features\")\n",
    "\n",
    "# disease_category is already categorical, include it\n",
    "remaining_categorical = [col for col in categorical_features if col in X.columns]\n",
    "if 'disease_category' in X.columns and 'disease_category' not in remaining_categorical:\n",
    "    remaining_categorical.append('disease_category')\n",
    "\n",
    "print(f\"   Features to one-hot encode: {remaining_categorical}\")\n",
    "\n",
    "if len(remaining_categorical) > 0:\n",
    "    # Combine train and test to ensure same columns\n",
    "    X_combined = pd.concat([X, X_test], keys=['train', 'test'])\n",
    "    \n",
    "    # One-hot encode\n",
    "    X_encoded = pd.get_dummies(\n",
    "        X_combined, \n",
    "        columns=remaining_categorical, \n",
    "        drop_first=True,\n",
    "        dtype=int\n",
    "    )\n",
    "    \n",
    "    # Split back\n",
    "    X = X_encoded.xs('train')\n",
    "    X_test = X_encoded.xs('test')\n",
    "    \n",
    "    n_new_features = len([col for col in X.columns if any(cat in col for cat in remaining_categorical)])\n",
    "    print(f\"   ✓ Created {n_new_features} binary features\")\n",
    "\n",
    "# --- Validation ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODING VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for remaining object columns\n",
    "object_cols_train = X.select_dtypes(include=['object']).columns.tolist()\n",
    "object_cols_test = X_test.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if object_cols_train or object_cols_test:\n",
    "    print(f\"  ⚠️ Warning: Still have object columns!\")\n",
    "    print(f\"    Train: {object_cols_train}\")\n",
    "    print(f\"    Test: {object_cols_test}\")\n",
    "else:\n",
    "    print(f\"  ✓ No object columns remain - all categorical features encoded\")\n",
    "\n",
    "# Check train/test consistency\n",
    "if list(X.columns) != list(X_test.columns):\n",
    "    print(f\"  ❌ ERROR: Train/test column mismatch after encoding!\")\n",
    "    train_only = set(X.columns) - set(X_test.columns)\n",
    "    test_only = set(X_test.columns) - set(X.columns)\n",
    "    if train_only:\n",
    "        print(f\"    Only in train: {train_only}\")\n",
    "    if test_only:\n",
    "        print(f\"    Only in test: {test_only}\")\n",
    "else:\n",
    "    print(f\"  ✓ Train and test have identical columns: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n  Final feature count: {X.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 7 COMPLETE - Categorical features encoded\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb9eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 8: MEDICAL FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "--- Creating Vital Sign Features ---\n",
      "  ✓ Pulse pressure\n",
      "  ✓ Systolic BP range\n",
      "  ✓ Shock index (clipped 0-3)\n",
      "  ✓ Modified shock index (clipped 0-3)\n",
      "  ✓ Hypoxemia indicator\n",
      "  ✓ Abnormal respiratory rate\n",
      "  ✓ Fever indicator\n",
      "  ✓ Hypothermia indicator\n",
      "  ✓ Temperature range\n",
      "  ✓ Hyperglycemia indicator\n",
      "  ✓ Hypoglycemia indicator\n",
      "  ✓ Glucose variability\n",
      "\n",
      "--- Creating Age-Based Features ---\n",
      "  ✓ Elderly indicator (>65)\n",
      "  ✓ Age squared\n",
      "  ✓ Age risk groups (one-hot encoded)\n",
      "  ✓ Heart rate range\n",
      "\n",
      "--- Creating Composite Severity Score ---\n",
      "  ✓ Severity score (0-5)\n",
      "    Distribution: {0: 6837, 1: 6834, 2: 4541, 3: 2088, 4: 537, 5: 48}\n",
      "\n",
      "--- Feature Engineering Summary ---\n",
      "  Features before: 75\n",
      "  Features after: 95\n",
      "  Features added: 20\n",
      "\n",
      "--- Binary Feature Validation ---\n",
      "  ✓ Hypoxemia: variance=0.1580, unique=2\n",
      "  ✓ RespRate_Abnormal: variance=0.2120, unique=2\n",
      "  ✓ Fever: variance=0.1424, unique=2\n",
      "  ✓ Hypothermia: variance=0.2331, unique=2\n",
      "  ✓ Hyperglycemia: variance=0.2307, unique=2\n",
      "  ✓ Hypoglycemia: variance=0.0757, unique=2\n",
      "  ✓ Elderly: variance=0.2292, unique=2\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 8 COMPLETE - Medical features engineered\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 8: MEDICAL FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 8: MEDICAL FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "original_feature_count = X.shape[1]\n",
    "\n",
    "print(\"\\n--- Creating Vital Sign Features ---\")\n",
    "\n",
    "# Blood Pressure Features\n",
    "if all(col in X.columns for col in ['SysBP_Mean', 'DiasBP_Mean']):\n",
    "    X['PulsePressure'] = X['SysBP_Mean'] - X['DiasBP_Mean']\n",
    "    X_test['PulsePressure'] = X_test['SysBP_Mean'] - X_test['DiasBP_Mean']\n",
    "    print(\"  ✓ Pulse pressure\")\n",
    "\n",
    "if all(col in X.columns for col in ['SysBP_Min', 'SysBP_Max']):\n",
    "    X['SysBP_Range'] = X['SysBP_Max'] - X['SysBP_Min']\n",
    "    X_test['SysBP_Range'] = X_test['SysBP_Max'] - X_test['SysBP_Min']\n",
    "    print(\"  ✓ Systolic BP range\")\n",
    "\n",
    "# Shock Indices (critical for ICU mortality)\n",
    "if all(col in X.columns for col in ['HeartRate_Mean', 'SysBP_Mean']):\n",
    "    X['ShockIndex'] = (X['HeartRate_Mean'] / (X['SysBP_Mean'] + 1)).clip(0, 3)\n",
    "    X_test['ShockIndex'] = (X_test['HeartRate_Mean'] / (X_test['SysBP_Mean'] + 1)).clip(0, 3)\n",
    "    print(\"  ✓ Shock index (clipped 0-3)\")\n",
    "\n",
    "if all(col in X.columns for col in ['HeartRate_Mean', 'MeanBP_Mean']):\n",
    "    X['ModifiedShockIndex'] = (X['HeartRate_Mean'] / (X['MeanBP_Mean'] + 1)).clip(0, 3)\n",
    "    X_test['ModifiedShockIndex'] = (X_test['HeartRate_Mean'] / (X_test['MeanBP_Mean'] + 1)).clip(0, 3)\n",
    "    print(\"  ✓ Modified shock index (clipped 0-3)\")\n",
    "\n",
    "# Respiratory Features\n",
    "if 'SpO2_Min' in X.columns:\n",
    "    X['Hypoxemia'] = (X['SpO2_Min'] < 90).astype(int)\n",
    "    X_test['Hypoxemia'] = (X_test['SpO2_Min'] < 90).astype(int)\n",
    "    print(\"  ✓ Hypoxemia indicator\")\n",
    "\n",
    "if 'RespRate_Mean' in X.columns:\n",
    "    X['RespRate_Abnormal'] = ((X['RespRate_Mean'] < 12) | (X['RespRate_Mean'] > 20)).astype(int)\n",
    "    X_test['RespRate_Abnormal'] = ((X_test['RespRate_Mean'] < 12) | (X_test['RespRate_Mean'] > 20)).astype(int)\n",
    "    print(\"  ✓ Abnormal respiratory rate\")\n",
    "\n",
    "# Temperature Features\n",
    "if 'TempC_Max' in X.columns:\n",
    "    X['Fever'] = (X['TempC_Max'] > 38).astype(int)\n",
    "    X_test['Fever'] = (X_test['TempC_Max'] > 38).astype(int)\n",
    "    print(\"  ✓ Fever indicator\")\n",
    "\n",
    "if 'TempC_Min' in X.columns:\n",
    "    X['Hypothermia'] = (X['TempC_Min'] < 36).astype(int)\n",
    "    X_test['Hypothermia'] = (X_test['TempC_Min'] < 36).astype(int)\n",
    "    print(\"  ✓ Hypothermia indicator\")\n",
    "\n",
    "if all(col in X.columns for col in ['TempC_Min', 'TempC_Max']):\n",
    "    X['Temp_Range'] = X['TempC_Max'] - X['TempC_Min']\n",
    "    X_test['Temp_Range'] = X_test['TempC_Max'] - X_test['TempC_Min']\n",
    "    print(\"  ✓ Temperature range\")\n",
    "\n",
    "# Glucose Features  \n",
    "if 'Glucose_Max' in X.columns:\n",
    "    X['Hyperglycemia'] = (X['Glucose_Max'] > 180).astype(int)\n",
    "    X_test['Hyperglycemia'] = (X_test['Glucose_Max'] > 180).astype(int)\n",
    "    print(\"  ✓ Hyperglycemia indicator\")\n",
    "\n",
    "if 'Glucose_Min' in X.columns:\n",
    "    X['Hypoglycemia'] = (X['Glucose_Min'] < 70).astype(int)\n",
    "    X_test['Hypoglycemia'] = (X_test['Glucose_Min'] < 70).astype(int)\n",
    "    print(\"  ✓ Hypoglycemia indicator\")\n",
    "\n",
    "if all(col in X.columns for col in ['Glucose_Min', 'Glucose_Max']):\n",
    "    X['Glucose_Range'] = X['Glucose_Max'] - X['Glucose_Min']\n",
    "    X_test['Glucose_Range'] = X_test['Glucose_Max'] - X_test['Glucose_Min']\n",
    "    print(\"  ✓ Glucose variability\")\n",
    "\n",
    "# Age Features\n",
    "print(\"\\n--- Creating Age-Based Features ---\")\n",
    "\n",
    "if 'age' in X.columns:\n",
    "    # Elderly indicator\n",
    "    X['Elderly'] = (X['age'] > 65).astype(int)\n",
    "    X_test['Elderly'] = (X_test['age'] > 65).astype(int)\n",
    "    print(\"  ✓ Elderly indicator (>65)\")\n",
    "    \n",
    "    # Age squared (non-linear effects)\n",
    "    X['age_squared'] = X['age'] ** 2\n",
    "    X_test['age_squared'] = X_test['age'] ** 2\n",
    "    print(\"  ✓ Age squared\")\n",
    "    \n",
    "    # Age risk groups\n",
    "    age_bins = [0, 18, 45, 65, 80, 120]\n",
    "    age_labels = ['pediatric', 'young_adult', 'middle_age', 'elderly', 'very_old']\n",
    "    \n",
    "    X['age_group'] = pd.cut(X['age'], bins=age_bins, labels=age_labels)\n",
    "    X_test['age_group'] = pd.cut(X_test['age'], bins=age_bins, labels=age_labels)\n",
    "    \n",
    "    # One-hot encode age groups\n",
    "    X_combined = pd.concat([X, X_test], keys=['train', 'test'])\n",
    "    X_encoded = pd.get_dummies(X_combined, columns=['age_group'], drop_first=True, prefix='age', dtype=int)\n",
    "    X = X_encoded.xs('train')\n",
    "    X_test = X_encoded.xs('test')\n",
    "    \n",
    "    print(\"  ✓ Age risk groups (one-hot encoded)\")\n",
    "\n",
    "# Heart Rate Variability\n",
    "if all(col in X.columns for col in ['HeartRate_Min', 'HeartRate_Max']):\n",
    "    X['HeartRate_Range'] = X['HeartRate_Max'] - X['HeartRate_Min']\n",
    "    X_test['HeartRate_Range'] = X_test['HeartRate_Max'] - X_test['HeartRate_Min']\n",
    "    print(\"  ✓ Heart rate range\")\n",
    "\n",
    "# Composite Severity Score\n",
    "print(\"\\n--- Creating Composite Severity Score ---\")\n",
    "\n",
    "severity_components = []\n",
    "\n",
    "if 'ShockIndex' in X.columns:\n",
    "    severity_components.append((X['ShockIndex'] > 0.9).astype(int))\n",
    "if 'Hypoxemia' in X.columns:\n",
    "    severity_components.append(X['Hypoxemia'])\n",
    "if 'RespRate_Abnormal' in X.columns:\n",
    "    severity_components.append(X['RespRate_Abnormal'])\n",
    "if 'Fever' in X.columns:\n",
    "    severity_components.append(X['Fever'])\n",
    "if 'Hypothermia' in X.columns:\n",
    "    severity_components.append(X['Hypothermia'])\n",
    "\n",
    "if severity_components:\n",
    "    X['Severity_Score'] = sum(severity_components)\n",
    "    \n",
    "    # Repeat for test\n",
    "    severity_components_test = []\n",
    "    if 'ShockIndex' in X_test.columns:\n",
    "        severity_components_test.append((X_test['ShockIndex'] > 0.9).astype(int))\n",
    "    if 'Hypoxemia' in X_test.columns:\n",
    "        severity_components_test.append(X_test['Hypoxemia'])\n",
    "    if 'RespRate_Abnormal' in X_test.columns:\n",
    "        severity_components_test.append(X_test['RespRate_Abnormal'])\n",
    "    if 'Fever' in X_test.columns:\n",
    "        severity_components_test.append(X_test['Fever'])\n",
    "    if 'Hypothermia' in X_test.columns:\n",
    "        severity_components_test.append(X_test['Hypothermia'])\n",
    "    \n",
    "    X_test['Severity_Score'] = sum(severity_components_test)\n",
    "    \n",
    "    print(f\"  ✓ Severity score (0-{len(severity_components)})\")\n",
    "    print(f\"    Distribution: {X['Severity_Score'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Summary\n",
    "new_feature_count = X.shape[1]\n",
    "added_features = new_feature_count - original_feature_count\n",
    "\n",
    "print(f\"\\n--- Feature Engineering Summary ---\")\n",
    "print(f\"  Features before: {original_feature_count}\")\n",
    "print(f\"  Features after: {new_feature_count}\")\n",
    "print(f\"  Features added: {added_features}\")\n",
    "\n",
    "# Validate binary features have variance\n",
    "print(\"\\n--- Binary Feature Validation ---\")\n",
    "\n",
    "binary_features = [\n",
    "    'Hypoxemia', 'RespRate_Abnormal', 'Fever', 'Hypothermia',\n",
    "    'Hyperglycemia', 'Hypoglycemia', 'Elderly'\n",
    "]\n",
    "\n",
    "for feat in binary_features:\n",
    "    if feat in X.columns:\n",
    "        var = X[feat].var()\n",
    "        unique = X[feat].nunique()\n",
    "        if var == 0 or unique == 1:\n",
    "            print(f\"  ⚠️ {feat}: variance={var:.4f}, unique={unique} (CONSTANT!)\")\n",
    "        else:\n",
    "            print(f\"  ✓ {feat}: variance={var:.4f}, unique={unique}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 8 COMPLETE - Medical features engineered\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d147f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 9: FEATURE SCALING (CORRECTED)\n",
      "================================================================================\n",
      "\n",
      "--- Identifying Features to Scale ---\n",
      "  Total numeric columns: 95\n",
      "  Binary indicator features: 16\n",
      "  Count features (DO NOT SCALE): 2\n",
      "  Ordinal features (DO NOT SCALE): 1\n",
      "  One-hot encoded features: 51\n",
      "\n",
      "  Features to scale: 35\n",
      "  Features to keep unscaled: 60\n",
      "\n",
      "--- Count Features (NOT scaling) ---\n",
      "  n_previous_icu_stays: range [0, 24], mean 0.4\n",
      "  n_diagnoses: range [1, 39], mean 14.8\n",
      "\n",
      "--- Sample Features ---\n",
      "  Scaling (first 10): ['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min', 'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean', 'MeanBP_Min']\n",
      "  Not scaling (first 10): ['n_previous_icu_stays', 'MARITAL_STATUS_WIDOWED', 'age_elderly', 'Hypoglycemia', 'has_aki', 'ETHNICITY_BLACK', 'Severity_Score', 'FIRST_CAREUNIT_TSICU', 'ETHNICITY_WHITE', 'Elderly']\n",
      "\n",
      "--- Applying StandardScaler ---\n",
      "  ✓ Scaled 35 continuous features\n",
      "  ✓ Left 60 features unscaled\n",
      "\n",
      "--- Scaling Validation ---\n",
      "\n",
      "  Checking scaled feature statistics (sample):\n",
      "    HeartRate_Min                  mean= 0.0000, std= 1.0000\n",
      "    HeartRate_Max                  mean= 0.0000, std= 1.0000\n",
      "    HeartRate_Mean                 mean= 0.0000, std= 1.0000\n",
      "    SysBP_Min                      mean= 0.0000, std= 1.0000\n",
      "    SysBP_Max                      mean=-0.0000, std= 1.0000\n",
      "\n",
      "  Checking count features remain integers:\n",
      "    n_previous_icu_stays      range=[0.0, 24.0], mean=0.4\n",
      "      ✓ Looks correct\n",
      "    n_diagnoses               range=[1.0, 39.0], mean=14.8\n",
      "      ✓ Looks correct\n",
      "\n",
      "  Checking binary features remain 0/1:\n",
      "    ✓ All binary features remain 0/1\n",
      "\n",
      "  Checking for invalid values after scaling:\n",
      "    NaN values: 0\n",
      "    Infinite values: 0\n",
      "    ✓ No invalid values\n",
      "\n",
      "  ✓ Train and test still have identical columns\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 9 COMPLETE - Features scaled CORRECTLY\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9: FEATURE SCALING (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 9: FEATURE SCALING (CORRECTED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Identifying Features to Scale ---\")\n",
    "\n",
    "# Get all numeric columns\n",
    "all_numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"  Total numeric columns: {len(all_numeric_cols)}\")\n",
    "\n",
    "# Features that should NOT be scaled\n",
    "binary_features_list = [\n",
    "    # Medical binary indicators\n",
    "    'Hypoxemia', 'RespRate_Abnormal', 'Fever', 'Hypothermia',\n",
    "    'Hyperglycemia', 'Hypoglycemia', 'Elderly',\n",
    "    \n",
    "    # Hospital history - binary flags\n",
    "    'is_first_icu_visit', \n",
    "    'is_frequent_flyer',\n",
    "    \n",
    "    # Condition flags - binary\n",
    "    'has_sepsis', 'has_heart_failure', 'has_respiratory_failure',\n",
    "    'has_aki', 'has_diabetes', 'has_copd', 'has_pneumonia'\n",
    "]\n",
    "\n",
    "# COUNT FEATURES - CRITICAL: DO NOT SCALE THESE!\n",
    "count_features = [\n",
    "    'n_previous_icu_stays',  # Count of previous ICU visits\n",
    "    'n_diagnoses'            # Count of diagnoses\n",
    "]\n",
    "\n",
    "# ORDINAL FEATURES - DO NOT SCALE\n",
    "ordinal_features = [\n",
    "    'Severity_Score'  # Ordinal score 0-5\n",
    "]\n",
    "\n",
    "# One-hot encoded features (all contain underscore and are binary)\n",
    "one_hot_features = [col for col in X.columns if '_' in col and X[col].nunique() <= 2]\n",
    "\n",
    "print(f\"  Binary indicator features: {len(binary_features_list)}\")\n",
    "print(f\"  Count features (DO NOT SCALE): {len(count_features)}\")\n",
    "print(f\"  Ordinal features (DO NOT SCALE): {len(ordinal_features)}\")\n",
    "print(f\"  One-hot encoded features: {len(one_hot_features)}\")\n",
    "\n",
    "# Combine ALL features to exclude from scaling\n",
    "exclude_from_scaling = list(set(\n",
    "    binary_features_list + \n",
    "    count_features + \n",
    "    ordinal_features + \n",
    "    one_hot_features\n",
    "))\n",
    "exclude_from_scaling = [col for col in exclude_from_scaling if col in all_numeric_cols]\n",
    "\n",
    "# Features to scale = numeric features - excluded features\n",
    "features_to_scale = [col for col in all_numeric_cols if col not in exclude_from_scaling]\n",
    "\n",
    "print(f\"\\n  Features to scale: {len(features_to_scale)}\")\n",
    "print(f\"  Features to keep unscaled: {len(exclude_from_scaling)}\")\n",
    "\n",
    "# Show what we're doing with count features\n",
    "print(f\"\\n--- Count Features (NOT scaling) ---\")\n",
    "for feat in count_features:\n",
    "    if feat in X.columns:\n",
    "        print(f\"  {feat}: range [{X[feat].min():.0f}, {X[feat].max():.0f}], mean {X[feat].mean():.1f}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n--- Sample Features ---\")\n",
    "print(f\"  Scaling (first 10): {features_to_scale[:10]}\")\n",
    "print(f\"  Not scaling (first 10): {exclude_from_scaling[:10]}\")\n",
    "\n",
    "# Scale continuous features only\n",
    "print(f\"\\n--- Applying StandardScaler ---\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X[features_to_scale] = scaler.fit_transform(X[features_to_scale])\n",
    "X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "print(f\"  ✓ Scaled {len(features_to_scale)} continuous features\")\n",
    "print(f\"  ✓ Left {len(exclude_from_scaling)} features unscaled\")\n",
    "\n",
    "# Validation\n",
    "print(f\"\\n--- Scaling Validation ---\")\n",
    "\n",
    "# Check 1: Scaled features should have mean ≈ 0, std ≈ 1\n",
    "print(f\"\\n  Checking scaled feature statistics (sample):\")\n",
    "sample_features = features_to_scale[:5]\n",
    "for feat in sample_features:\n",
    "    mean = X[feat].mean()\n",
    "    std = X[feat].std()\n",
    "    print(f\"    {feat:30s} mean={mean:7.4f}, std={std:7.4f}\")\n",
    "    \n",
    "    if abs(mean) > 0.1:\n",
    "        print(f\"      ⚠️ Mean not close to 0!\")\n",
    "    if abs(std - 1.0) > 0.1:\n",
    "        print(f\"      ⚠️ Std not close to 1!\")\n",
    "\n",
    "# Check 2: Count features should still be integers\n",
    "print(f\"\\n  Checking count features remain integers:\")\n",
    "for feat in count_features:\n",
    "    if feat in X.columns:\n",
    "        min_val = X[feat].min()\n",
    "        max_val = X[feat].max()\n",
    "        mean_val = X[feat].mean()\n",
    "        print(f\"    {feat:25s} range=[{min_val:.1f}, {max_val:.1f}], mean={mean_val:.1f}\")\n",
    "        \n",
    "        if min_val < -1:  # If negative, it was scaled!\n",
    "            print(f\"      ❌ ERROR: This was scaled! Values should be positive counts!\")\n",
    "        else:\n",
    "            print(f\"      ✓ Looks correct\")\n",
    "\n",
    "# Check 3: Binary features should still be 0/1\n",
    "print(f\"\\n  Checking binary features remain 0/1:\")\n",
    "binary_check_passed = True\n",
    "\n",
    "for feat in binary_features_list:\n",
    "    if feat in X.columns:\n",
    "        unique_vals = set(X[feat].unique())\n",
    "        if not unique_vals.issubset({0, 1, 0.0, 1.0}):\n",
    "            print(f\"    ❌ {feat}: values are {sorted(unique_vals)[:5]}\")\n",
    "            binary_check_passed = False\n",
    "\n",
    "if binary_check_passed:\n",
    "    print(f\"    ✓ All binary features remain 0/1\")\n",
    "\n",
    "# Check 4: No NaN or Inf introduced by scaling\n",
    "nan_count = X.isnull().sum().sum()\n",
    "inf_count = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"\\n  Checking for invalid values after scaling:\")\n",
    "print(f\"    NaN values: {nan_count}\")\n",
    "print(f\"    Infinite values: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(f\"    ❌ ERROR: Scaling introduced invalid values!\")\n",
    "else:\n",
    "    print(f\"    ✓ No invalid values\")\n",
    "\n",
    "# Check 5: Train and test still have same columns\n",
    "if list(X.columns) != list(X_test.columns):\n",
    "    print(f\"\\n  ❌ ERROR: Column mismatch after scaling!\")\n",
    "else:\n",
    "    print(f\"\\n  ✓ Train and test still have identical columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 9 COMPLETE - Features scaled CORRECTLY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9015da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 10: FINAL VALIDATION & SAVE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE FINAL VALIDATION\n",
      "================================================================================\n",
      "\n",
      "1. Shape Consistency\n",
      "   X_train: (20885, 95)\n",
      "   y_train: (20885,)\n",
      "   X_test: (5221, 95)\n",
      "   test_ids: 5221\n",
      "   ✓ X and y have matching samples: 20885\n",
      "   ✓ Train and test have matching features: 95\n",
      "   ✓ Test set and test_ids match: 5221\n",
      "\n",
      "2. Missing Values\n",
      "   X_train: 0 missing\n",
      "   y_train: 0 missing\n",
      "   X_test: 0 missing\n",
      "   ✓ No missing values\n",
      "\n",
      "3. Infinite Values\n",
      "   X_train: 0 infinite\n",
      "   X_test: 0 infinite\n",
      "   ✓ No infinite values\n",
      "\n",
      "4. Column Consistency\n",
      "   ✓ Train and test have identical column names\n",
      "\n",
      "5. Data Types\n",
      "   ✓ All features are numeric\n",
      "\n",
      "6. Target Distribution\n",
      "   Mortality rate: 0.112 (2345/20885)\n",
      "   Class 0: 18540\n",
      "   Class 1: 2345\n",
      "   ✓ Mortality rate matches expected (~11.2%)\n",
      "\n",
      "7. Feature Value Ranges (sample)\n",
      "   age: [-2.25, 2.15] (scaled)\n",
      "   ShockIndex: [-3.11, 8.91] (scaled)\n",
      "   ModifiedShockIndex: [-3.24, 8.34] (scaled)\n",
      "   Severity_Score: [0.00, 5.00] ✓\n",
      "\n",
      "8. Binary Features Have Variance\n",
      "   ✓ Hypoxemia: variance=0.1580, unique=2\n",
      "   ✓ Fever: variance=0.1424, unique=2\n",
      "   ✓ Elderly: variance=0.2292, unique=2\n",
      "   ✓ is_first_icu_visit: variance=0.1709, unique=2\n",
      "\n",
      "================================================================================\n",
      "✅ ALL VALIDATION CHECKS PASSED!\n",
      "================================================================================\n",
      "\n",
      "🎉 Data is ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10: FINAL VALIDATION AND SAVE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 10: FINAL VALIDATION & SAVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE FINAL VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "validation_passed = True\n",
    "issues = []\n",
    "\n",
    "# ============================================================================\n",
    "# Check 1: Shape Consistency\n",
    "# ============================================================================\n",
    "print(\"\\n1. Shape Consistency\")\n",
    "print(f\"   X_train: {X.shape}\")\n",
    "print(f\"   y_train: {y.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   test_ids: {len(test_ids)}\")\n",
    "\n",
    "if X.shape[0] != y.shape[0]:\n",
    "    issues.append(\"❌ X and y have different number of samples\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"   ✓ X and y have matching samples: {X.shape[0]}\")\n",
    "\n",
    "if X.shape[1] != X_test.shape[1]:\n",
    "    issues.append(\"❌ X_train and X_test have different number of features\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"   ✓ Train and test have matching features: {X.shape[1]}\")\n",
    "\n",
    "if X_test.shape[0] != len(test_ids):\n",
    "    issues.append(\"❌ X_test and test_ids have different lengths\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"   ✓ Test set and test_ids match: {len(test_ids)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 2: No Missing Values\n",
    "# ============================================================================\n",
    "print(\"\\n2. Missing Values\")\n",
    "\n",
    "X_missing = X.isnull().sum().sum()\n",
    "y_missing = y.isnull().sum()\n",
    "X_test_missing = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"   X_train: {X_missing} missing\")\n",
    "print(f\"   y_train: {y_missing} missing\")\n",
    "print(f\"   X_test: {X_test_missing} missing\")\n",
    "\n",
    "if X_missing > 0 or y_missing > 0 or X_test_missing > 0:\n",
    "    issues.append(\"❌ Missing values present\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"   ✓ No missing values\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 3: No Infinite Values\n",
    "# ============================================================================\n",
    "print(\"\\n3. Infinite Values\")\n",
    "\n",
    "X_inf = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()\n",
    "X_test_inf = np.isinf(X_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "print(f\"   X_train: {X_inf} infinite\")\n",
    "print(f\"   X_test: {X_test_inf} infinite\")\n",
    "\n",
    "if X_inf > 0 or X_test_inf > 0:\n",
    "    issues.append(\"❌ Infinite values present\")\n",
    "    validation_passed = False\n",
    "else:\n",
    "    print(f\"   ✓ No infinite values\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 4: Column Names Match\n",
    "# ============================================================================\n",
    "print(\"\\n4. Column Consistency\")\n",
    "\n",
    "if list(X.columns) == list(X_test.columns):\n",
    "    print(f\"   ✓ Train and test have identical column names\")\n",
    "else:\n",
    "    issues.append(\"❌ Column names don't match\")\n",
    "    validation_passed = False\n",
    "    \n",
    "    train_only = set(X.columns) - set(X_test.columns)\n",
    "    test_only = set(X_test.columns) - set(X.columns)\n",
    "    \n",
    "    if train_only:\n",
    "        print(f\"   Columns only in train: {train_only}\")\n",
    "    if test_only:\n",
    "        print(f\"   Columns only in test: {test_only}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 5: Data Types\n",
    "# ============================================================================\n",
    "print(\"\\n5. Data Types\")\n",
    "\n",
    "# All should be numeric\n",
    "non_numeric_train = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "non_numeric_test = X_test.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "if non_numeric_train or non_numeric_test:\n",
    "    issues.append(\"❌ Non-numeric columns present\")\n",
    "    validation_passed = False\n",
    "    print(f\"   Non-numeric in train: {non_numeric_train}\")\n",
    "    print(f\"   Non-numeric in test: {non_numeric_test}\")\n",
    "else:\n",
    "    print(f\"   ✓ All features are numeric\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 6: Target Distribution\n",
    "# ============================================================================\n",
    "print(\"\\n6. Target Distribution\")\n",
    "\n",
    "target_mean = y.mean()\n",
    "target_count = y.sum()\n",
    "\n",
    "print(f\"   Mortality rate: {target_mean:.3f} ({target_count}/{len(y)})\")\n",
    "print(f\"   Class 0: {(y==0).sum()}\")\n",
    "print(f\"   Class 1: {(y==1).sum()}\")\n",
    "\n",
    "expected_mortality = 0.112\n",
    "if abs(target_mean - expected_mortality) > 0.01:\n",
    "    issues.append(f\"⚠️ Target distribution changed: {target_mean:.3f} vs expected {expected_mortality:.3f}\")\n",
    "else:\n",
    "    print(f\"   ✓ Mortality rate matches expected (~11.2%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 7: Feature Value Ranges\n",
    "# ============================================================================\n",
    "print(\"\\n7. Feature Value Ranges (sample)\")\n",
    "\n",
    "# Check a few critical features\n",
    "critical_features = {\n",
    "    'age': (0, 120),\n",
    "    'ShockIndex': (0, 3),\n",
    "    'ModifiedShockIndex': (0, 3),\n",
    "    'Severity_Score': (0, 5)\n",
    "}\n",
    "\n",
    "for feat, (expected_min, expected_max) in critical_features.items():\n",
    "    if feat in X.columns:\n",
    "        actual_min = X[feat].min()\n",
    "        actual_max = X[feat].max()\n",
    "        \n",
    "        # For scaled features, ranges will be different\n",
    "        if feat in features_to_scale:\n",
    "            print(f\"   {feat}: [{actual_min:.2f}, {actual_max:.2f}] (scaled)\")\n",
    "        else:\n",
    "            print(f\"   {feat}: [{actual_min:.2f}, {actual_max:.2f}]\", end=\"\")\n",
    "            \n",
    "            if actual_min < expected_min or actual_max > expected_max:\n",
    "                print(f\" ⚠️ outside expected [{expected_min}, {expected_max}]\")\n",
    "            else:\n",
    "                print(f\" ✓\")\n",
    "\n",
    "# ============================================================================\n",
    "# Check 8: Binary Features\n",
    "# ============================================================================\n",
    "print(\"\\n8. Binary Features Have Variance\")\n",
    "\n",
    "for feat in ['Hypoxemia', 'Fever', 'Elderly', 'is_first_icu_visit']:\n",
    "    if feat in X.columns:\n",
    "        var = X[feat].var()\n",
    "        unique = X[feat].nunique()\n",
    "        \n",
    "        if var == 0:\n",
    "            issues.append(f\"❌ {feat} has zero variance\")\n",
    "            validation_passed = False\n",
    "            print(f\"   ❌ {feat}: variance={var:.4f}, unique={unique}\")\n",
    "        else:\n",
    "            print(f\"   ✓ {feat}: variance={var:.4f}, unique={unique}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL VERDICT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if validation_passed and len(issues) == 0:\n",
    "    print(\"✅ ALL VALIDATION CHECKS PASSED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n🎉 Data is ready for modeling!\")\n",
    "    \n",
    "else:\n",
    "    print(\"🚨 VALIDATION ISSUES FOUND\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nIssues:\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"  {i}. {issue}\")\n",
    "    \n",
    "    print(\"\\n⚠️ Review and fix issues before training models!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ed9ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "✓ Saved processed datasets to ..\\data\\processed_final/\n",
      "✓ Saved preprocessing objects\n",
      "✓ Saved feature metadata\n",
      "✓ Saved preprocessing summary\n",
      "\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 20,885\n",
      "  Test samples: 5,221\n",
      "  Total features: 95\n",
      "\n",
      "Feature Breakdown:\n",
      "  Continuous (scaled): 35\n",
      "  Binary (unscaled): 60\n",
      "\n",
      "Target Distribution:\n",
      "  Mortality rate: 0.112\n",
      "  Deaths: 2345 / 20885\n",
      "\n",
      "Feature Groups:\n",
      "  - Original vitals: ~24\n",
      "  - Hospital history: 3\n",
      "  - ICD9 diagnoses: ~7\n",
      "  - Condition flags: 7\n",
      "  - Engineered vitals: ~13\n",
      "  - Age features: ~6\n",
      "  - One-hot encoded: ~35\n",
      "\n",
      "Validation Status:  ALL CHECKS PASSED\n",
      "\n",
      "Files Saved:\n",
      "  - X_train_processed.pkl\n",
      "  - y_train.pkl\n",
      "  - X_test_processed.pkl\n",
      "  - test_ids.pkl\n",
      "  - numeric_imputer.pkl\n",
      "  - categorical_imputer.pkl\n",
      "  - scaler.pkl\n",
      "  - feature_info.pkl\n",
      "\n",
      "Ready for modeling! \n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ SECTION 10 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING PIPELINE COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "if validation_passed:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVING PROCESSED DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    import os\n",
    "    save_dir = Path(\"../data/processed_final\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save datasets\n",
    "    X.to_pickle(save_dir / 'X_train_processed.pkl')\n",
    "    y.to_pickle(save_dir / 'y_train.pkl')\n",
    "    X_test.to_pickle(save_dir / 'X_test_processed.pkl')\n",
    "    test_ids.to_pickle(save_dir / 'test_ids.pkl')\n",
    "    \n",
    "    print(f\"\\n✓ Saved processed datasets to {save_dir}/\")\n",
    "    \n",
    "    # Save preprocessing objects\n",
    "    with open(save_dir / 'numeric_imputer.pkl', 'wb') as f:\n",
    "        pickle.dump(numeric_imputer, f)\n",
    "    with open(save_dir / 'categorical_imputer.pkl', 'wb') as f:\n",
    "        pickle.dump(categorical_imputer, f)\n",
    "    with open(save_dir / 'scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"✓ Saved preprocessing objects\")\n",
    "    \n",
    "    # Save feature lists for reference\n",
    "    feature_info = {\n",
    "        'all_features': X.columns.tolist(),\n",
    "        'features_to_scale': features_to_scale,\n",
    "        'binary_features': exclude_from_scaling,\n",
    "        'n_features': X.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open(save_dir / 'feature_info.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_info, f)\n",
    "    \n",
    "    print(f\"✓ Saved feature metadata\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary = f\"\"\"\n",
    "PREPROCESSING SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Dataset Statistics:\n",
    "  Training samples: {X.shape[0]:,}\n",
    "  Test samples: {X_test.shape[0]:,}\n",
    "  Total features: {X.shape[1]}\n",
    "  \n",
    "Feature Breakdown:\n",
    "  Continuous (scaled): {len(features_to_scale)}\n",
    "  Binary (unscaled): {len(exclude_from_scaling)}\n",
    "  \n",
    "Target Distribution:\n",
    "  Mortality rate: {y.mean():.3f}\n",
    "  Deaths: {y.sum()} / {len(y)}\n",
    "  \n",
    "Feature Groups:\n",
    "  - Original vitals: ~24\n",
    "  - Hospital history: 3\n",
    "  - ICD9 diagnoses: ~7\n",
    "  - Condition flags: 7\n",
    "  - Engineered vitals: ~13\n",
    "  - Age features: ~6\n",
    "  - One-hot encoded: ~35\n",
    "\n",
    "Validation Status:  ALL CHECKS PASSED\n",
    "\n",
    "Files Saved:\n",
    "  - X_train_processed.pkl\n",
    "  - y_train.pkl\n",
    "  - X_test_processed.pkl\n",
    "  - test_ids.pkl\n",
    "  - numeric_imputer.pkl\n",
    "  - categorical_imputer.pkl\n",
    "  - scaler.pkl\n",
    "  - feature_info.pkl\n",
    "\n",
    "Ready for modeling! \n",
    "\"\"\"\n",
    "    \n",
    "    with open(save_dir / 'preprocessing_summary.txt', 'w') as f:\n",
    "        f.write(summary)\n",
    "    \n",
    "    print(f\"✓ Saved preprocessing summary\")\n",
    "    \n",
    "    print(summary)\n",
    "\n",
    "else:\n",
    "    print(\"\\n Data NOT saved due to validation failures\")\n",
    "    print(\"   Fix the issues above and re-run preprocessing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ SECTION 10 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classification _HEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
