{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e14a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20885, 44), (5221, 39))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../data/\")   # full path to the HEF folder\n",
    "\n",
    "train = pd.read_csv(data_path / \"mimic_train_HEF.csv\", low_memory=False)\n",
    "test  = pd.read_csv(data_path / \"mimic_test_HEF.csv\",  low_memory=False)\n",
    "\n",
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b95932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved 5221 test IDs for submission\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE TEST IDs FIRST (BEFORE DROPPING!)\n",
    "# =============================================================================\n",
    "\n",
    "test_ids = test['icustay_id'].copy()\n",
    "print(f\"\\n✓ Saved {len(test_ids)} test IDs for submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7e4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DROP LEAKAGE COLUMNS\n",
    "# =============================================================================\n",
    "columns_to_drop = [\n",
    "    'DISCHTIME', 'DEATHTIME', 'DOD', 'LOS',\n",
    "    'subject_id', 'hadm_id', 'icustay_id',\n",
    "    'ADMITTIME', 'Diff'\n",
    "]\n",
    "\n",
    "train_clean = train.drop(columns=columns_to_drop, errors='ignore')\n",
    "test_clean = test.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599153bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. SEPARATE TARGET\n",
    "# =============================================================================\n",
    "y = train_clean['HOSPITAL_EXPIRE_FLAG']\n",
    "X = train_clean.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "X_test = test_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357cffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric features: 24\n",
      "Categorical features: 10\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. IDENTIFY FEATURE TYPES\n",
    "# =============================================================================\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05dd99c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Imputing missing values ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. IMPUTATION\n",
    "# =============================================================================\n",
    "print(\"\\n--- Imputing missing values ---\")\n",
    "\n",
    "# Numeric\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])\n",
    "X_test[numeric_features] = numeric_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# Categorical\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
    "X_test[categorical_features] = categorical_imputer.transform(X_test[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d663e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Converting DOB to age ---\n",
      "  ✓ Calculated age from DOB and ADMITTIME\n",
      "    Age range: 15.0 - 89.0 years\n",
      "    Mean age: 62.7 years\n",
      "    Missing ages: 1107\n",
      "    ✓ Imputed invalid ages with median: 64.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_13476\\1596579972.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['age'].fillna(age_median, inplace=True)\n",
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_13476\\1596579972.py:68: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test['age'].fillna(age_median, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1: Convert DOB to age (handling MIMIC-III date shifting)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 1: Converting DOB to age ---\")\n",
    "\n",
    "if 'DOB' in X.columns and 'DOB' in categorical_features:\n",
    "    # MIMIC-III shifts all dates forward by ~200 years for anonymization\n",
    "    # But the RELATIVE age is preserved\n",
    "    # Strategy: Calculate age = ADMITTIME - DOB\n",
    "    \n",
    "    # Reload original data to get ADMITTIME\n",
    "    train_original = pd.read_csv('../data/mimic_train_HEF.csv')\n",
    "    test_original = pd.read_csv('../data/mimic_test_HEF.csv')\n",
    "    \n",
    "    # Convert to datetime\n",
    "    dob_train = pd.to_datetime(X['DOB'], errors='coerce')\n",
    "    dob_test = pd.to_datetime(X_test['DOB'], errors='coerce')\n",
    "    admit_train = pd.to_datetime(train_original['ADMITTIME'], errors='coerce')\n",
    "    admit_test = pd.to_datetime(test_original['ADMITTIME'], errors='coerce')\n",
    "    \n",
    "    # Calculate age using timedelta and convert to years\n",
    "    # Use .apply() to avoid overflow\n",
    "    def calculate_age(admit_time, dob):\n",
    "        if pd.isna(admit_time) or pd.isna(dob):\n",
    "            return np.nan\n",
    "        try:\n",
    "            # Calculate difference in days, then convert to years\n",
    "            age_days = (admit_time - dob).days\n",
    "            age_years = age_days / 365.25\n",
    "            return age_years\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # Calculate age for train\n",
    "    X['age'] = [calculate_age(admit, dob) for admit, dob in zip(admit_train, dob_train)]\n",
    "    \n",
    "    # Calculate age for test\n",
    "    X_test['age'] = [calculate_age(admit, dob) for admit, dob in zip(admit_test, dob_test)]\n",
    "    \n",
    "    # Convert to numeric (in case of any issues)\n",
    "    X['age'] = pd.to_numeric(X['age'], errors='coerce')\n",
    "    X_test['age'] = pd.to_numeric(X_test['age'], errors='coerce')\n",
    "    \n",
    "    # Clean up\n",
    "    X = X.drop('DOB', axis=1)\n",
    "    X_test = X_test.drop('DOB', axis=1)\n",
    "    categorical_features.remove('DOB')\n",
    "    \n",
    "    print(f\"  ✓ Calculated age from DOB and ADMITTIME\")\n",
    "    print(f\"    Age range: {X['age'].min():.1f} - {X['age'].max():.1f} years\")\n",
    "    print(f\"    Mean age: {X['age'].mean():.1f} years\")\n",
    "    print(f\"    Missing ages: {X['age'].isna().sum()}\")\n",
    "    \n",
    "    # Sanity check: ages should be reasonable (0-120 years)\n",
    "    if X['age'].max() > 120 or X['age'].min() < 0:\n",
    "        print(f\"    ⚠️ WARNING: Unusual age range detected!\")\n",
    "        print(f\"    Sample ages: {X['age'].head(10).tolist()}\")\n",
    "    \n",
    "    # Handle missing or invalid ages\n",
    "    if X['age'].isna().sum() > 0 or (X['age'] < 0).any() or (X['age'] > 120).any():\n",
    "        # Set invalid ages to NaN\n",
    "        X.loc[(X['age'] < 0) | (X['age'] > 120), 'age'] = np.nan\n",
    "        X_test.loc[(X_test['age'] < 0) | (X_test['age'] > 120), 'age'] = np.nan\n",
    "        \n",
    "        # Impute with median\n",
    "        age_median = X['age'].median()\n",
    "        X['age'].fillna(age_median, inplace=True)\n",
    "        X_test['age'].fillna(age_median, inplace=True)\n",
    "        print(f\"    ✓ Imputed invalid ages with median: {age_median:.1f}\")\n",
    "    \n",
    "    # Add to numeric features for scaling later\n",
    "    if 'age' not in numeric_features:\n",
    "        numeric_features.append('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ef4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DIAGNOSING CATEGORICAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "Total categorical features: 9\n",
      "  GENDER: 2 unique values\n",
      "    Distribution: {'M': 11759, 'F': 9126}\n",
      "  ADMISSION_TYPE: 3 unique values\n",
      "    Distribution: {'EMERGENCY': 17817, 'ELECTIVE': 2848, 'URGENT': 220}\n",
      "  INSURANCE: 5 unique values\n",
      "    Distribution: {'Medicare': 11718, 'Private': 6245, 'Medicaid': 2117, 'Government': 611, 'Self Pay': 194}\n",
      "  RELIGION: 17 unique values\n",
      "    ⚠️ HIGH CARDINALITY - will create 17 one-hot columns!\n",
      "  MARITAL_STATUS: 7 unique values\n",
      "    Distribution: {'MARRIED': 10386, 'SINGLE': 5910, 'WIDOWED': 2819, 'DIVORCED': 1413, 'SEPARATED': 240, 'UNKNOWN (DEFAULT)': 103, 'LIFE PARTNER': 14}\n",
      "  ETHNICITY: 41 unique values\n",
      "    ⚠️ HIGH CARDINALITY - will create 41 one-hot columns!\n",
      "  DIAGNOSIS: 6193 unique values\n",
      "    ⚠️ HIGH CARDINALITY - will create 6193 one-hot columns!\n",
      "  ICD9_diagnosis: 1853 unique values\n",
      "    ⚠️ HIGH CARDINALITY - will create 1853 one-hot columns!\n",
      "  FIRST_CAREUNIT: 5 unique values\n",
      "    Distribution: {'MICU': 8640, 'SICU': 3961, 'CSRU': 3127, 'TSICU': 2645, 'CCU': 2512}\n",
      "\n",
      "⚠️ ESTIMATED TOTAL FEATURES AFTER ONE-HOT ENCODING: 8142\n",
      "Current numeric features: 25\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5.5. DIAGNOSE HIGH-CARDINALITY CATEGORICAL FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSING CATEGORICAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Check cardinality (number of unique values) for each categorical feature\n",
    "for cat_col in categorical_features:\n",
    "    n_unique = X[cat_col].nunique()\n",
    "    print(f\"  {cat_col}: {n_unique} unique values\")\n",
    "    \n",
    "    # Show distribution if few unique values\n",
    "    if n_unique <= 10:\n",
    "        print(f\"    Distribution: {X[cat_col].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(f\"    ⚠️ HIGH CARDINALITY - will create {n_unique} one-hot columns!\")\n",
    "\n",
    "# Estimate final feature count after one-hot encoding\n",
    "estimated_features = len(numeric_features)\n",
    "for cat_col in categorical_features:\n",
    "    estimated_features += X[cat_col].nunique() - 1  # -1 because drop_first=True\n",
    "\n",
    "print(f\"\\n⚠️ ESTIMATED TOTAL FEATURES AFTER ONE-HOT ENCODING: {estimated_features}\")\n",
    "print(f\"Current numeric features: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233ba060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WHITE', 'BLACK/AFRICAN AMERICAN', 'BLACK/CAPE VERDEAN',\n",
       "       'UNKNOWN/NOT SPECIFIED', 'PATIENT DECLINED TO ANSWER',\n",
       "       'ASIAN - ASIAN INDIAN', 'OTHER', 'HISPANIC/LATINO - PUERTO RICAN',\n",
       "       'ASIAN', 'HISPANIC OR LATINO', 'UNABLE TO OBTAIN', 'BLACK/HAITIAN',\n",
       "       'WHITE - OTHER EUROPEAN',\n",
       "       'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER', 'WHITE - RUSSIAN',\n",
       "       'WHITE - EASTERN EUROPEAN', 'ASIAN - CHINESE',\n",
       "       'HISPANIC/LATINO - CUBAN', 'ASIAN - VIETNAMESE',\n",
       "       'MULTI RACE ETHNICITY', 'AMERICAN INDIAN/ALASKA NATIVE',\n",
       "       'MIDDLE EASTERN', 'ASIAN - KOREAN', 'CARIBBEAN ISLAND',\n",
       "       'PORTUGUESE', 'HISPANIC/LATINO - SALVADORAN', 'ASIAN - FILIPINO',\n",
       "       'HISPANIC/LATINO - GUATEMALAN', 'ASIAN - CAMBODIAN',\n",
       "       'HISPANIC/LATINO - DOMINICAN', 'WHITE - BRAZILIAN',\n",
       "       'HISPANIC/LATINO - CENTRAL AMERICAN (OTHER)',\n",
       "       'HISPANIC/LATINO - HONDURAN', 'HISPANIC/LATINO - MEXICAN',\n",
       "       'BLACK/AFRICAN', 'ASIAN - JAPANESE', 'HISPANIC/LATINO - COLOMBIAN',\n",
       "       'AMERICAN INDIAN/ALASKA NATIVE FEDERALLY RECOGNIZED TRIBE',\n",
       "       'ASIAN - OTHER', 'SOUTH AMERICAN', 'ASIAN - THAI'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['ETHNICITY'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50b3ee87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PROTESTANT QUAKER', 'UNOBTAINABLE', 'NOT SPECIFIED', 'JEWISH',\n",
       "       'CATHOLIC', 'OTHER', 'BUDDHIST', 'EPISCOPALIAN',\n",
       "       'ROMANIAN EAST. ORTH', 'GREEK ORTHODOX', \"JEHOVAH'S WITNESS\",\n",
       "       'MUSLIM', 'CHRISTIAN SCIENTIST', 'HINDU', '7TH DAY ADVENTIST',\n",
       "       'UNITARIAN-UNIVERSALIST', 'HEBREW'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['RELIGION'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa1720ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SINGLE', 'MARRIED', 'SEPARATED', 'WIDOWED', 'DIVORCED',\n",
       "       'UNKNOWN (DEFAULT)', 'LIFE PARTNER'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['MARITAL_STATUS'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95243011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART CATEGORICAL ENCODING\n",
      "======================================================================\n",
      "\n",
      "--- Step 2: Processing ICD9_diagnosis codes ---\n",
      "  ✓ Extracted ICD9 categories: 530 unique categories\n",
      "  → Using target encoding for ICD9 categories\n",
      "  ✓ ICD9_diagnosis → ICD9_encoded (numeric)\n",
      "\n",
      "--- Step 3: Processing DIAGNOSIS (free text) ---\n",
      "  ✓ Dropping DIAGNOSIS (free text, 6193 unique values)\n",
      "    → Keeping ICD9_encoded instead (more structured)\n",
      "\n",
      "--- Step 4: Grouping ETHNICITY ---\n",
      "  ✓ Grouped ETHNICITY: 41 → 8 categories\n",
      "    New categories: ['ASIAN', 'BLACK', 'HISPANIC', 'NATIVE', 'OTHER', 'PACIFIC_ISLANDER', 'UNKNOWN', 'WHITE']\n",
      "    Distribution:\n",
      "      WHITE: 15330 (73.4%)\n",
      "      BLACK: 2201 (10.5%)\n",
      "      UNKNOWN: 1320 (6.3%)\n",
      "      HISPANIC: 852 (4.1%)\n",
      "      OTHER: 616 (2.9%)\n",
      "      ASIAN: 545 (2.6%)\n",
      "      NATIVE: 15 (0.1%)\n",
      "      PACIFIC_ISLANDER: 6 (0.0%)\n",
      "\n",
      "--- Step 5: Grouping RELIGION ---\n",
      "  ✓ Grouped RELIGION: 17 → 8 categories\n",
      "    New categories: ['CATHOLIC', 'JEWISH', 'MUSLIM', 'ORTHODOX', 'OTHER', 'OTHER_RELIGION', 'PROTESTANT', 'UNKNOWN']\n",
      "    Distribution:\n",
      "      CATHOLIC: 7655 (36.7%)\n",
      "      UNKNOWN: 6913 (33.1%)\n",
      "      PROTESTANT: 3041 (14.6%)\n",
      "      JEWISH: 1841 (8.8%)\n",
      "      OTHER: 743 (3.6%)\n",
      "      OTHER_RELIGION: 440 (2.1%)\n",
      "      ORTHODOX: 178 (0.9%)\n",
      "      MUSLIM: 74 (0.4%)\n",
      "\n",
      "--- Step 6: Grouping MARITAL_STATUS ---\n",
      "  ✓ Grouped MARITAL_STATUS: 7 → 5 categories\n",
      "    New categories: ['DIVORCED_SEPARATED', 'MARRIED', 'SINGLE', 'UNKNOWN', 'WIDOWED']\n",
      "    Distribution:\n",
      "      MARRIED: 10400 (49.8%)\n",
      "      SINGLE: 5910 (28.3%)\n",
      "      WIDOWED: 2819 (13.5%)\n",
      "      DIVORCED_SEPARATED: 1653 (7.9%)\n",
      "      UNKNOWN: 103 (0.5%)\n",
      "\n",
      "--- Step 7: One-hot encoding remaining categorical features ---\n",
      "\n",
      "Features to one-hot encode (7):\n",
      "  GENDER: 2 categories → 1 binary features\n",
      "  ADMISSION_TYPE: 3 categories → 2 binary features\n",
      "  INSURANCE: 5 categories → 4 binary features\n",
      "  RELIGION: 8 categories → 7 binary features\n",
      "  MARITAL_STATUS: 5 categories → 4 binary features\n",
      "  ETHNICITY: 8 categories → 7 binary features\n",
      "  FIRST_CAREUNIT: 5 categories → 4 binary features\n",
      "\n",
      "Estimated new binary features from one-hot encoding: 29\n",
      "✓ One-hot encoding complete\n",
      "\n",
      "======================================================================\n",
      "ENCODING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "Original numeric features: 26\n",
      "Target-encoded features: 1 (ICD9_encoded)\n",
      "Binary features from one-hot encoding: 29\n",
      "\n",
      "Final shapes:\n",
      "  X: (20885, 55)\n",
      "  X_test: (5221, 55)\n",
      "  Total features: 55\n",
      "\n",
      "✓ Feature count looks good (55 features)\n",
      "\n",
      "Sample of final features (first 20):\n",
      "['HeartRate_Min', 'HeartRate_Max', 'HeartRate_Mean', 'SysBP_Min', 'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean', 'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min', 'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean', 'SpO2_Min', 'SpO2_Max']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. SMART CATEGORICAL ENCODING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMART CATEGORICAL ENCODING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 2: Handle ICD9_diagnosis (extract category, then target encode)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 2: Processing ICD9_diagnosis codes ---\")\n",
    "\n",
    "if 'ICD9_diagnosis' in X.columns:\n",
    "    # ICD9 codes have hierarchical structure:\n",
    "    # First digit = broad category (e.g., 4XX = circulatory system)\n",
    "    # First 3 digits = more specific category\n",
    "    \n",
    "    def extract_icd9_category(code):\n",
    "        \"\"\"Extract first 3 characters from ICD9 code\"\"\"\n",
    "        if pd.isna(code):\n",
    "            return 'UNKNOWN'\n",
    "        code_str = str(code).strip()\n",
    "        # Remove decimal point and take first 3 characters\n",
    "        code_str = code_str.replace('.', '')\n",
    "        if len(code_str) >= 3:\n",
    "            return code_str[:3]\n",
    "        elif len(code_str) > 0:\n",
    "            return code_str\n",
    "        else:\n",
    "            return 'UNKNOWN'\n",
    "    \n",
    "    X['ICD9_category'] = X['ICD9_diagnosis'].apply(extract_icd9_category)\n",
    "    X_test['ICD9_category'] = X_test['ICD9_diagnosis'].apply(extract_icd9_category)\n",
    "    \n",
    "    n_icd9_categories = X['ICD9_category'].nunique()\n",
    "    print(f\"  ✓ Extracted ICD9 categories: {n_icd9_categories} unique categories\")\n",
    "    \n",
    "    # Target encode (because still likely 100+ categories)\n",
    "    print(f\"  → Using target encoding for ICD9 categories\")\n",
    "    encoding_map = y.groupby(X['ICD9_category']).mean().to_dict()\n",
    "    global_mean = y.mean()\n",
    "    \n",
    "    X['ICD9_encoded'] = X['ICD9_category'].map(encoding_map)\n",
    "    X_test['ICD9_encoded'] = X_test['ICD9_category'].map(encoding_map).fillna(global_mean)\n",
    "    \n",
    "    # Add to numeric features (target encoding creates numeric feature)\n",
    "    numeric_features.append('ICD9_encoded')\n",
    "    \n",
    "    # Drop originals\n",
    "    X = X.drop(['ICD9_diagnosis', 'ICD9_category'], axis=1)\n",
    "    X_test = X_test.drop(['ICD9_diagnosis', 'ICD9_category'], axis=1)\n",
    "    categorical_features.remove('ICD9_diagnosis')\n",
    "    \n",
    "    print(f\"  ✓ ICD9_diagnosis → ICD9_encoded (numeric)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 3: Handle DIAGNOSIS (free text - extract keywords or drop)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 3: Processing DIAGNOSIS (free text) ---\")\n",
    "\n",
    "if 'DIAGNOSIS' in X.columns:\n",
    "    # Option 1: Drop it (safest - free text is very high cardinality)\n",
    "    # Option 2: Extract common keywords (more complex)\n",
    "    \n",
    "    # For now, let's DROP it to keep things simple\n",
    "    # (We already have ICD9 codes which are more structured)\n",
    "    \n",
    "    print(f\"  ✓ Dropping DIAGNOSIS (free text, {X['DIAGNOSIS'].nunique()} unique values)\")\n",
    "    print(f\"    → Keeping ICD9_encoded instead (more structured)\")\n",
    "    \n",
    "    X = X.drop('DIAGNOSIS', axis=1)\n",
    "    X_test = X_test.drop('DIAGNOSIS', axis=1)\n",
    "    categorical_features.remove('DIAGNOSIS')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 4: Group ETHNICITY into broader categories\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 4: Grouping ETHNICITY ---\")\n",
    "\n",
    "if 'ETHNICITY' in X.columns:\n",
    "    def group_ethnicity(ethnicity):\n",
    "        if pd.isna(ethnicity):\n",
    "            return 'UNKNOWN'\n",
    "        ethnicity = str(ethnicity).upper()\n",
    "        \n",
    "        # WHITE (includes variants like WHITE - RUSSIAN, WHITE - BRAZILIAN, etc.)\n",
    "        if 'WHITE' in ethnicity:\n",
    "            return 'WHITE'\n",
    "        \n",
    "        # BLACK (includes BLACK/AFRICAN AMERICAN, BLACK/HAITIAN, BLACK/CAPE VERDEAN, etc.)\n",
    "        elif 'BLACK' in ethnicity or 'AFRICAN' in ethnicity:\n",
    "            return 'BLACK'\n",
    "        \n",
    "        # HISPANIC/LATINO (all variants)\n",
    "        elif 'HISPANIC' in ethnicity or 'LATINO' in ethnicity:\n",
    "            return 'HISPANIC'\n",
    "        \n",
    "        # ASIAN (includes ASIAN - CHINESE, ASIAN - VIETNAMESE, etc.)\n",
    "        elif 'ASIAN' in ethnicity:\n",
    "            return 'ASIAN'\n",
    "        \n",
    "        # NATIVE/INDIGENOUS (American Indian/Alaska Native)\n",
    "        elif 'AMERICAN INDIAN' in ethnicity or 'ALASKA NATIVE' in ethnicity:\n",
    "            return 'NATIVE'\n",
    "        \n",
    "        # PACIFIC ISLANDER\n",
    "        elif 'HAWAIIAN' in ethnicity or 'PACIFIC ISLANDER' in ethnicity:\n",
    "            return 'PACIFIC_ISLANDER'\n",
    "        \n",
    "        # UNKNOWN/NOT SPECIFIED/DECLINED\n",
    "        elif any(x in ethnicity for x in ['UNKNOWN', 'UNABLE', 'DECLINED', 'NOT SPECIFIED']):\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        # OTHER (includes MULTI RACE, MIDDLE EASTERN, CARIBBEAN, PORTUGUESE, etc.)\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    X['ETHNICITY'] = X['ETHNICITY'].apply(group_ethnicity)\n",
    "    X_test['ETHNICITY'] = X_test['ETHNICITY'].apply(group_ethnicity)\n",
    "    \n",
    "    print(f\"  ✓ Grouped ETHNICITY: 41 → {X['ETHNICITY'].nunique()} categories\")\n",
    "    print(f\"    New categories: {sorted(X['ETHNICITY'].unique())}\")\n",
    "    print(f\"    Distribution:\")\n",
    "    for cat, count in X['ETHNICITY'].value_counts().items():\n",
    "        print(f\"      {cat}: {count} ({count/len(X)*100:.1f}%)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 5: Group RELIGION into broader categories\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 5: Grouping RELIGION ---\")\n",
    "\n",
    "if 'RELIGION' in X.columns:\n",
    "    def group_religion(religion):\n",
    "        if pd.isna(religion):\n",
    "            return 'UNKNOWN'\n",
    "        religion = str(religion).upper()\n",
    "        \n",
    "        # CATHOLIC\n",
    "        if 'CATHOLIC' in religion:\n",
    "            return 'CATHOLIC'\n",
    "        \n",
    "        # PROTESTANT/CHRISTIAN (includes PROTESTANT QUAKER, EPISCOPALIAN, etc.)\n",
    "        elif any(x in religion for x in ['PROTESTANT', 'EPISCOPALIAN', 'QUAKER']):\n",
    "            return 'PROTESTANT'\n",
    "        \n",
    "        # JEWISH (includes HEBREW)\n",
    "        elif 'JEWISH' in religion or 'HEBREW' in religion:\n",
    "            return 'JEWISH'\n",
    "        \n",
    "        # MUSLIM\n",
    "        elif 'MUSLIM' in religion:\n",
    "            return 'MUSLIM'\n",
    "        \n",
    "        # ORTHODOX (GREEK ORTHODOX, ROMANIAN ORTHODOX)\n",
    "        elif 'ORTHODOX' in religion:\n",
    "            return 'ORTHODOX'\n",
    "        \n",
    "        # OTHER RELIGIONS (Buddhist, Hindu, Christian Scientist, Jehovah's Witness, etc.)\n",
    "        elif any(x in religion for x in ['BUDDHIST', 'HINDU', 'JEHOVAH', 'CHRISTIAN SCIENTIST', \n",
    "                                          '7TH DAY ADVENTIST', 'UNITARIAN']):\n",
    "            return 'OTHER_RELIGION'\n",
    "        \n",
    "        # UNKNOWN/NOT SPECIFIED\n",
    "        elif any(x in religion for x in ['UNOBTAINABLE', 'NOT SPECIFIED', 'UNKNOWN']):\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        # OTHER\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    X['RELIGION'] = X['RELIGION'].apply(group_religion)\n",
    "    X_test['RELIGION'] = X_test['RELIGION'].apply(group_religion)\n",
    "    \n",
    "    print(f\"  ✓ Grouped RELIGION: 17 → {X['RELIGION'].nunique()} categories\")\n",
    "    print(f\"    New categories: {sorted(X['RELIGION'].unique())}\")\n",
    "    print(f\"    Distribution:\")\n",
    "    for cat, count in X['RELIGION'].value_counts().items():\n",
    "        print(f\"      {cat}: {count} ({count/len(X)*100:.1f}%)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 6: Group MARITAL_STATUS\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 6: Grouping MARITAL_STATUS ---\")\n",
    "\n",
    "if 'MARITAL_STATUS' in X.columns:\n",
    "    def group_marital_status(status):\n",
    "        if pd.isna(status):\n",
    "            return 'UNKNOWN'\n",
    "        status = str(status).upper()\n",
    "        \n",
    "        # MARRIED (includes LIFE PARTNER)\n",
    "        if 'MARRIED' in status or 'LIFE PARTNER' in status:\n",
    "            return 'MARRIED'\n",
    "        \n",
    "        # SINGLE\n",
    "        elif 'SINGLE' in status:\n",
    "            return 'SINGLE'\n",
    "        \n",
    "        # WIDOWED\n",
    "        elif 'WIDOWED' in status:\n",
    "            return 'WIDOWED'\n",
    "        \n",
    "        # DIVORCED/SEPARATED (group together - both indicate ended relationship)\n",
    "        elif 'DIVORCED' in status or 'SEPARATED' in status:\n",
    "            return 'DIVORCED_SEPARATED'\n",
    "        \n",
    "        # UNKNOWN\n",
    "        elif 'UNKNOWN' in status:\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        else:\n",
    "            return 'UNKNOWN'\n",
    "    \n",
    "    X['MARITAL_STATUS'] = X['MARITAL_STATUS'].apply(group_marital_status)\n",
    "    X_test['MARITAL_STATUS'] = X_test['MARITAL_STATUS'].apply(group_marital_status)\n",
    "    \n",
    "    print(f\"  ✓ Grouped MARITAL_STATUS: 7 → {X['MARITAL_STATUS'].nunique()} categories\")\n",
    "    print(f\"    New categories: {sorted(X['MARITAL_STATUS'].unique())}\")\n",
    "    print(f\"    Distribution:\")\n",
    "    for cat, count in X['MARITAL_STATUS'].value_counts().items():\n",
    "        print(f\"      {cat}: {count} ({count/len(X)*100:.1f}%)\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 7: One-hot encode remaining low-cardinality features\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n--- Step 7: One-hot encoding remaining categorical features ---\")\n",
    "\n",
    "# Update categorical_features list\n",
    "remaining_categorical = [col for col in categorical_features if col in X.columns]\n",
    "print(f\"\\nFeatures to one-hot encode ({len(remaining_categorical)}):\")\n",
    "\n",
    "# Verify cardinality\n",
    "total_new_features = 0\n",
    "for col in remaining_categorical:\n",
    "    n_unique = X[col].nunique()\n",
    "    total_new_features += (n_unique - 1)  # drop_first=True\n",
    "    print(f\"  {col}: {n_unique} categories → {n_unique-1} binary features\")\n",
    "\n",
    "print(f\"\\nEstimated new binary features from one-hot encoding: {total_new_features}\")\n",
    "\n",
    "if len(remaining_categorical) > 0:\n",
    "    # One-hot encode\n",
    "    X_combined = pd.concat([X, X_test], keys=['train', 'test'])\n",
    "    X_encoded = pd.get_dummies(X_combined, columns=remaining_categorical, drop_first=True)\n",
    "    X = X_encoded.xs('train')\n",
    "    X_test = X_encoded.xs('test')\n",
    "    \n",
    "    print(f\"✓ One-hot encoding complete\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENCODING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original numeric features: {len(numeric_features)}\")\n",
    "print(f\"Target-encoded features: 1 (ICD9_encoded)\")\n",
    "print(f\"Binary features from one-hot encoding: {total_new_features}\")\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  Total features: {X.shape[1]}\")\n",
    "\n",
    "# Check if reasonable\n",
    "if X.shape[1] > 200:\n",
    "    print(f\"\\n⚠️ WARNING: {X.shape[1]} features might still be too many\")\n",
    "    print(\"Consider more aggressive grouping or feature selection\")\n",
    "elif X.shape[1] < 50:\n",
    "    print(f\"\\n⚠️ WARNING: Only {X.shape[1]} features - might be too few\")\n",
    "    print(\"Consider keeping more granular categories\")\n",
    "else:\n",
    "    print(f\"\\n✓ Feature count looks good ({X.shape[1]} features)\")\n",
    "\n",
    "# Show a sample of the final feature names\n",
    "print(f\"\\nSample of final features (first 20):\")\n",
    "print(list(X.columns[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4743b5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaling numeric features ---\n",
      "\n",
      "Final shapes:\n",
      "  X: (20885, 55)\n",
      "  y: (20885,)\n",
      "  X_test: (5221, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_13476\\613314864.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numeric_cols_to_scale] = scaler.fit_transform(X[numeric_cols_to_scale])\n",
      "C:\\Users\\corne\\AppData\\Local\\Temp\\ipykernel_13476\\613314864.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. FEATURE SCALING\n",
    "# =============================================================================\n",
    "print(\"--- Scaling numeric features ---\")\n",
    "\n",
    "numeric_cols_to_scale = [col for col in numeric_features if col in X.columns]\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols_to_scale] = scaler.fit_transform(X[numeric_cols_to_scale])\n",
    "X_test[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  y: {y.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40a4fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving processed data ---\n",
      "✓ Processed data saved to ../data/processed/\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "You can now run modeling notebooks without repeating preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 8. SAVE PROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n--- Saving processed data ---\")\n",
    "\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save as pickle (preserves dtypes and column names)\n",
    "X.to_pickle('../data/processed/X_train_processed.pkl')\n",
    "y.to_pickle('../data/processed/y_train.pkl')\n",
    "X_test.to_pickle('../data/processed/X_test_processed.pkl')\n",
    "test_ids.to_pickle('../data/processed/test_ids.pkl')\n",
    "\n",
    "# Also save preprocessing objects (to use on new data if needed)\n",
    "with open('../data/processed/numeric_imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(numeric_imputer, f)\n",
    "with open('../data/processed/categorical_imputer.pkl', 'wb') as f:\n",
    "    pickle.dump(categorical_imputer, f)\n",
    "with open('../data/processed/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✓ Processed data saved to ../data/processed/\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now run modeling notebooks without repeating preprocessing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Classification _HEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
